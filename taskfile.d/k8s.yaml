version: '3'

tasks:

  anteon:deploy:self:
    cmds:
      - helm repo add anteon https://getanteon.github.io/anteon-helm-charts/
      - helm repo update
      # Anteon platform
      - >
        helm upgrade --install
        anteon-selfhosted anteon/anteon
        --namespace anteon --create-namespace
        --wait
        --reuse-values

  anteon:deploy:alaz:
    requires:
      vars: [MONITORING_ID]
    cmds:
      - helm repo add anteon https://getanteon.github.io/anteon-helm-charts/
      - helm repo update
      # eBPF agent (Alaz)
      - >
        helm upgrade --install
        alaz anteon/alaz
        --namespace anteon --create-namespace
        --set monitoringID={{.MONITORING_ID}}

  apisix:deploy:self:
    cmds:
      - task: apisix:deploy:bitnami

  apisix:deploy:bitnami:
    vars:
      _INGRESS: >-
        {{if ne INGRESS "no" -}}
        --set ingressController.enabled=true
        {{- end}}
      _DASHBOARD: >-
        {{if ne DASHBOARD "no" -}}
        --set dashboard.enabled=true
        {{- end}}
    cmds:
      - helm repo add --force-update apisix https://charts.apiseven.com
      - >-
        helm upgrade --install
        apisix oci://registry-1.docker.io/bitnamicharts/apisix
        --version 0.14.0
        --create-namespace --namespace ingress-apisix
        --reuse-values
        {{._INGRESS}}
        {{._DASHBOARD}}
        {{.CLI_ARGS}}

  apisix:deploy:ingress:
    vars:
      _GATEWAY_API: >-
        {{if ne GATEWAY_API "no" -}}
        --set config.kubernetes.enableGatewayAPI=true
        {{- end}}
    cmds:
      - helm repo add --force-update apisix https://charts.apiseven.com
      - >-
        helm upgrade --install
        apisix-ingress-controller apisix/apisix-ingress-controller
        --version 0.14.0
        --create-namespace --namespace ingress-apisix
        --reuse-values
        {{._GATEWAY_API}}
        {{.CLI_ARGS}}

  apisix:deploy:normal:
    cmds:
      - helm repo add --force-update apisix https://charts.apiseven.com
      - >-
        helm upgrade --install
        apisix apisix/apisix
        --version 2.9.0
        --create-namespace  --namespace apisix
        --reuse-values
        {{.CLI_ARGS}}

  apisix:deploy:dashboard:
    cmds:
      - helm repo add --force-update apisix https://charts.apiseven.com
      - >-
        helm upgrade --install
        apisix-dashboard apisix/apisix-dashboard
        --version 0.8.2
        --create-namespace --namespace apisix
        --reuse-values
        {{.CLI_ARGS}}

  argocd:deploy:self:helm:
    desc: 'Deploy ArgoCD controllers to cluster [CONTEXT]'
    vars:
      _CONTEXT: '{{if .CONTEXT}}--kube-context {{.CONTEXT}}{{end}}'
    cmds:
      - helm repo add argo https://argoproj.github.io/argo-helm
      - helm repo update
      - helm upgrade --install argocd argo/argo-cd --namespace argocd --create-namespace {{._CONTEXT}}

  argocd:deploy:self:kubectl:
    desc: 'Deploy ArgoCD controllers without HA to cluster [CONTEXT]'
    vars:
      _CONTEXT: '{{if .CONTEXT}}--context {{.CONTEXT}}{{end}}'
    cmds:
      - kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml {{._CONTEXT}}

  argocd:admin:get-initial-password:
    desc: 'Print initial admin password i.e. secret "argocd-initial-admin-secret" base64 encoded'
    cmds:
      - argocd admin initial-password -n argocd

  argocd:admin:passwd:
    desc: 'Change admin password'
    cmds:
      - argocd account update-password

  carina:deploy:self:
    vars:
      _VERSION: 'v0.11.0'
    cmds:
      - helm repo add carina-csi-driver https://carina-io.github.io
      - helm repo update
      - >
        helm upgrade --install
        carina-csi-driver carina-csi-driver/carina-csi-driver
        --version {{._VERSION}}
        --namespace kube-system --create-namespace

  cert-manager:deploy:self:
    vars:
      _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}v1.15.3{{end}}'
    cmds:
      - helm repo add jetstack https://charts.jetstack.io --force-update
      - >
        helm upgrade --install
        cert-manager jetstack/cert-manager
        --version {{._VERSION}}
        --namespace cert-manager --create-namespace
        --set crds.enabled=true
        --set "extraArgs={--enable-gateway-api}"

  cert-manager:deploy:csi:
    vars:
      _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}v0.10.1{{end}}'
    cmds:
      - helm repo add jetstack https://charts.jetstack.io --force-update
      - >
        helm upgrade --install
        cert-manager-csi-driver jetstack/cert-manager-csi-driver
        --version {{._VERSION}}
        --namespace cert-manager --create-namespace
        {{.CLI_ARGS}}

  cilium:deploy:self:k3s:
    desc: >-
      Deploy cilium
      [

      ]
      -- [options]
    vars:
      _VERSION: '{{default "1.16.2" .VERSION}}'
      # Default IPv4 Pod CIDR is 10.0.0.0/8
      # If your node network is in the same range,
      # you will lose connectivity to other nodes.
      _POD_CIDR: >-
        {{if .POD_CIDR -}}
        --set ipam.clusterPoolIPv4PodCIDRList[0]={{.POD_CIDR}}
        {{- end}}
      _ISTIO: >-
        {{if .ISTIO -}}
        --set kubeProxyReplacement=true
        --set socketLB.hostNamespaceOnly=true
        {{- end}}
      _INGRESS: >-
        {{if eq INGRESS "yes" -}}
        --set ingressController.enabled=true
        --set ingressController.loadbalancerMode=dedicated
        --set ingressController.default=true
        {{- end}}
      _EGRESS: >-
        {{if eq EGRESS "yes" -}}
        --set egressGateway.enabled=true
        --set bpf.masquerade=true
        --set kubeProxyReplacement=true
        {{- end}}
      _GATEWAY_API: >-
        {{if eq GATEWAY_API "yes" -}}
        --set kubeProxyReplacement=true
        --set gatewayAPI.enabled=true
        {{- end}}
      _NODE_PORT: >-
        {{if eq NODE_PORT "yes" -}}
        --set nodePort.enabled=true
        {{- end}}
      _HOST_PORT: >-
        {{if eq HOST_PORT "yes" -}}
        --set hostPort.enabled=true
        {{- end}}
      _IPAM_LB: >-
        {{if eq IPAM_LB "yes" -}}
        --set nodeIPAM.enabled=true
        {{- end}}
      _SOCKET_LB: >-
        {{if eq LB "yes" -}}
        --set externalIPs.enabled=true
        {{- end}}
      _SOCKET_LB: >-
        {{if eq SOCKET_LB "yes" -}}
        --set socketLB.enabled=true
        --set socketLB.hostNamespaceOnly=true
        {{- end}}
      _FIREWALL: >-
        {{if eq FIREWALL "yes" -}}
        --set hostFirewall.enabled=true
        {{- end}}
      _PROMETHEUS: >-
        {{if eq PROMETHEUS "yes" -}}
        --set prometheus.enabled=true
        --set operator.prometheus.enabled=true
        {{- end}}
      _GRAFANA: >-
        {{if eq GRAFANA "yes" -}}
        --set dashboards.enabled=true
        {{- end}}
    cmds:
      - helm repo add cilium https://helm.cilium.io/
      - helm repo update
      - >
        helm upgrade --install
        cilium cilium/cilium
        --version {{._VERSION}}
        --namespace kube-system
        --reuse-values
        {{._POD_CIDR}}
        {{._INGRESS}}
        {{._EGRESS}}
        {{._GATEWAY_API}}
        {{._PROMETHEUS}}
        {{._GRAFANA}}
        {{.CLI_ARGS}}
      # Restart unmanaged Pods
      - >-
        kubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,HOSTNETWORK:.spec.hostNetwork --no-headers=true
        | grep '<none>'
        | awk '{print "-n "$1" "$2}'
        | xargs -L 1 -r kubectl delete pod
      - >-
        {{if eq ._INGRESS "yes" -}}
        kubectl -n kube-system rollout restart deployment/cilium-operator;
        kubectl -n kube-system rollout restart ds/cilium;
        {{else if eq ._GATEWAY_API "yes" -}}
        kubectl -n kube-system rollout restart deployment/cilium-operator;
        kubectl -n kube-system rollout restart ds/cilium;
        {{else if eq ._IPAM "yes" -}}
        kubectl -n kube-system rollout restart deployment/cilium-operator;
        {{- end}}

  cilium:check:
    cmds:
      - cilium status --wait
      - cilium connectivity test

  cilium:k3s:stop:
    desc: >-
      Before stopping k3s cluster,
      you MUST remove cilium network interfaces and iptables rules.
      If you fail to do this,
      you may lose network connectivity to the host when K3s is stopped
    cmds:
      - ip link delete cilium_host
      - ip link delete cilium_net
      - ip link delete cilium_vxlan
      - iptables-save | grep -iv cilium | iptables-restore
      - ip6tables-save | grep -iv cilium | ip6tables-restore

  cnpg:deploy:self:
    cmds:
      - helm repo add cnpg https://cloudnative-pg.github.io/charts --force-update
      - >-
        helm upgrade --install
        cnpg cnpg/cloudnative-pg
        --namespace cnpg-system --create-namespace
        --reuse-values
        {{.CLI_ARGS}}

  cnpg:deploy:cluster:
    desc: >-
      Deploy postgres cluster
      [
      RELEASE
      NAMESPACE
      POSTGIS
      PG_VERSION
      INSTANCES
      SIZE
      STORAGE_CLASS
      S3
      AWS_ENDPOINTS
      AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_BUCKET
      AWS_REGION
      AWS_PATH
      CRON
      RETENTION
      ]
      -- [
      helm_upgrade_options
      ]
    vars:
      _NAME: '{{default "postgres" .NAME}}'
      _NAMESPACE: '{{default "database" .NAMESPACE}}'
      _POSTGIS: >-
        {{if eq .POSTGIS "yes" -}}
        --set type=postgis
        {{- end}}
      _PG_VERSION: '--set-string version.postgresql={{default 16 .PG_VERSION}}'
      # `cluster` mode
      _INSTANCES: '--set cluster.instances={{default 1 .INSTANCES}}'
      _SIZE: '--set cluster.storage.size={{default "8Gi" .SIZE}}'
      _STORAGE_CLASS: >-
        {{if .STORAGE_CLASS -}}
        --set cluster.storage.storageClass={{.STORAGE_CLASS}}
        {{- end}}
      # Backup
      _S3: >-
        {{if eq .S3 "yes" -}}
        --set backups.enabled=true
        --set backups.provider=s3
        --set backups.endpointURL={{.AWS_ENDPOINTS}}
        --set backups.s3.region="{{default "" .AWS_REGION}}"
        --set backups.s3.accessKey={{.AWS_ACCESS_KEY_ID}}
        --set backups.s3.secretKey={{.AWS_SECRET_ACCESS_KEY}}
        --set backups.s3.bucket={{.AWS_BUCKET}}
        --set backups.s3.path="{{default "/" .AWS_PATH}}"
        --set backups.scheduledBackups[0].name=daily-backup
        --set backups.scheduledBackups[0].backupOwnerReference=self
        --set backups.scheduledBackups[0].method=barmanObjectStore
        --set backups.scheduledBackups[0].schedule="{{default "0 0 3 * * *" .CRON}}"
        --set backups.retentionPolicy={{default "30d" .RETENTION}}
        {{- end}}
    cmds:
      - helm repo add cnpg https://cloudnative-pg.github.io/charts --force-update
      - >
        helm upgrade --install
        {{._NAME}} cnpg/cluster
        --namespace {{._NAMESPACE}} --create-namespace
        --reuse-values
        {{._POSTGIS}}
        {{._PG_VERSION}}
        {{._INSTANCES}}
        {{._SIZE}}
        {{._STORAGE_CLASS}}
        {{._S3}}
        {{.CLI_ARGS}}

  consul:deploy:self:
    cmds:
      - helm repo add hashicorp https://helm.releases.hashicorp.com --force-update
      - >-
        helm upgrade --install
        consul hashicorp/consul
        --version 1.5.3
        --create-namespace --namespace consul
        --reuse-values
        --set global.name=consul
        --set connectInject.enabled=true
        --set ingressGateways.enabled=ture
        {{.CLI_ARGS}}

  csi:nfs:deploy:self:
    _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}v4.9.0{{end}}'
    cmds:
      - helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts
      - helm repo update
      - >-
        helm upgrade --install
        csi-driver-nfs csi-driver-nfs/csi-driver-nfs
        --version {{._VERSION}}
        --namespace kube-system
        --set storageClass.create=true
        {{.CLI_ARGS}}

  csi:s3:deploy:self:
    requires:
      vars:
        - S3_ENDPOINT
        - ACCESS_KEY_ID
        - SECRET_ACCESS_KEY
        - S3_BUCKET
    vars:
      _S3_REGION: >-
        {{if .S3_REGION -}}
        --set secret.region={{.S3_REGION}}
        {{- end}}
      _RELEASE: '{{default "csi-s3" .RELEASE}}'
    cmds:
      - helm repo add yandex-s3 https://yandex-cloud.github.io/k8s-csi-s3/charts
      - helm repo update
      - >
        helm upgrade --install
        {{._RELEASE}} yandex-s3/csi-s3
        -n csi-s3 --create-namespace
        {{._S3_REGION}}
        --set secret.endpoint={{.S3_ENDPOINT}}
        --set secret.accessKey={{.ACCESS_KEY_ID}}
        --set secret.secretKey={{.SECRET_ACCESS_KEY}}
        --set storageClass.singleBucket={{.S3_BUCKET}}

  csi:smb:deploy:self:
    _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}v1.16.0{{end}}'
    cmds:
      - helm repo add csi-driver-smb https://raw.githubusercontent.com/kubernetes-csi/csi-driver-smb/master/charts
      - helm repo update
      - >-
        helm upgrade --install
        csi-driver-smb csi-driver-smb/csi-driver-smb
        --version {{._VERSION}}
        --namespace kube-system
        {{.CLI_ARGS}}

  csi:seasweedfs:deploy:self:
    _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}0.2.2{{end}}'
    cmds:
      - helm repo add --force-update seaweedfs-csi-driver https://seaweedfs.github.io/seaweedfs-csi-driver/helm
      - >-
        helm upgrade --install
        seaweedfs-csi-driver seaweedfs-csi-driver/seaweedfs-csi-driver
        --version {{._VERSION}}
        {{.CLI_ARGS}}

  envoy-gateway:deploy:self:
    desc: 'Deploy Envoy Gateway that supports gwapi v1.1.0'
    cmds:
      - >-
        helm upgrade --install
        eg oci://docker.io/envoyproxy/gateway-helm
        --version v1.1.2
        --namespace envoy-gateway-system --create-namespace
        --reuse-values
        {{.CLI_ARGS}}
      - >-
        kubectl wait
        --timeout=5m
        --namespace envoy-gateway-system
        deployment/envoy-gateway
        --for=condition=Available

  envoy-gateway:deploy:gateway:web:
    desc: >-
      [
      GATEWAY_SERVICE_TYPE
      ]
    cmds:
      - |-
        kubectl apply -f - <<EOF
        {{if .GATEWAY_SERVICE_TYPE}}
        ---
        apiVersion: gateway.envoyproxy.io/v1alpha1
        kind: EnvoyProxy
        metadata:
          name: custom-eg-config
          namespace: envoy-gateway-system
        spec:
          provider:
            type: Kubernetes
            kubernetes:
              envoyService:
                type: {{.GATEWAY_SERVICE_TYPE}}
        {{end}}
        ---
        apiVersion: gateway.networking.k8s.io/v1
        kind: GatewayClass
        metadata:
          name: eg
        spec:
          controllerName: gateway.envoyproxy.io/gatewayclass-controller
          {{- if .GATEWAY_SERVICE_TYPE}}
          parametersRef:
            group: gateway.envoyproxy.io
            kind: EnvoyProxy
            name: custom-eg-config
            namespace: envoy-gateway-system
          {{- end}}
        ---
        apiVersion: gateway.networking.k8s.io/v1
        kind: Gateway
        metadata:
          name: eg-web
          namespace: envoy-gateway-system
        spec:
          gatewayClassName: eg
          listeners:
            - name: http
              protocol: HTTP
              port: 80
            - name: https
              protocol: HTTPS
              port: 443
        EOF

  fission:deploy:self:
    vars:
      _VERSION: 'v1.20.4'
      _NAMESPACE: fission
    cmds:
      - kubectl create namespace {{._NAMESPACE}}
      - kubectl create -k "github.com/fission/fission/crds/v1?ref={{._VERSION}}"
      - helm repo add fission-charts https://fission.github.io/fission-charts/
      - helm repo update
      - >
        helm upgrade --install
        fission fission-charts/fission-all
        --version {{._VERSION}}
        --namespace {{._NAMESPACE}} --create-namespace

  flux:deploy:self:
    desc: 'Deploy Flux CD controllers to cluster [CONTEXT]'
    preconditions:
      - flux check --pre
    vars:
      _CONTEXT: '{{if .CONTEXT}}--context {{.CONTEXT}}{{end}}'
    cmds:
      - flux install {{._CONTEXT}}

  flux:deploy:community:
    desc: 'Deploy Flux CD controllers to cluster [NAME CONTEXT]'
    preconditions:
      - helm version
      - flux check --pre
    vars:
      _NAME: '{{if .NAME}}{{.NAME}}{{else}}flux{{end}}'
      _CONTEXT: '{{if .CONTEXT}}--kube-context {{.CONTEXT}}{{end}}'
    cmds:
      - helm repo add fluxcd-community https://fluxcd-community.github.io/helm-charts
      - >
        helm upgrade --install
        {{._CONTEXT}}
        --namespace flux-system --create-namespace
        {{._NAME}} fluxcd-community/flux2
      - flux check

  gwapi:install:v1.1.0:
    cmds:
      - kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml

  helm:repo:add:
    desc: 'Add a repo <REPO>'
    internal: true
    requires: {vars: [REPO]}
    cmds:
      - cmd: 'helm repo add {{.REPO}}'

  helm:repo:add:bulk:
    desc: 'Add repos <REPOS>'
    internal: true
    requires: {var: [REPOS]}
    cmds:
      - task: 'helm:repo:add'
        vars: {REPO: '{{.REPO}}'}
        for: {var: REPOS, as: REPO}
      - cmd: helm repo update

  helm:repo:add:curated-list:
    desc: 'Import repos'
    cmds:
      - task: 'helm:repo:add:bulk'
        vars:
          REPOS:
            - anteon https://getanteon.github.io/anteon-helm-charts/
            - argo https://argoproj.github.io/argo-helm
            - authentik https://charts.goauthentik.io
            - bitnami https://charts.bitnami.com/bitnami
            - cnpg https://cloudnative-pg.github.io/charts/
            - datawire https://app.getambassador.io
            - fission-charts https://fission.github.io/fission-charts/
            - fluxcd https://charts.fluxcd.io
            - fluxcd-community https://fluxcd-community.github.io/helm-charts
            - istio https://istio-release.storage.googleapis.com/charts
            - jetstack https://charts.jetstack.io
            - kanister https://charts.kanister.io/
            - kong https://charts.konghq.com
            - kubeshark https://helm.kubeshark.co
            - longhorn https://charts.longhorn.io
            - traefik https://traefik.github.io/charts
            - openebs https://openebs.github.io/openebs
            - openfaas https://openfaas.github.io/faas-netes/
            - prometheus-community https://prometheus-community.github.io/helm-charts
            - vmware-tanzu https://vmware-tanzu.github.io/helm-charts

  hwameistor:deploy:self:
    cmds:
      - helm repo add hwameistor-operator https://hwameistor.io/hwameistor-operator
      - helm repo update
      - >
        helm upgrade --install
        hwameistor-operator hwameistor-operator/hwameistor-operator
        --namespace hwameistor --create-namespace
        {{.CLI_ARGS}}

  istio:deploy:self:
    cmds:
      - >-
        helm upgrade --install
        istio-base istio/base
        -n istio-system --create-namespace
        --wait
      - >-
        helm upgrade --install
        istiod istio/istiod
        -n istio-system --create-namespace
        --set profile=ambient
        --wait
      - >-
        helm upgrade --install
        istio-ingress istio/gateway
        -n istio-system --create-namespace
        --wait

  istio:deploy:self:default:
    cmds:
      # Install the control plane
      - helm repo add istio https://istio-release.storage.googleapis.com/charts
      - helm repo update
      - helm upgrade --install -n istio-system --create-namespace istio-base istio/base --wait
      - kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml
      - helm upgrade --install -n istio-system istiod istio/istiod --wait
      # Install the data plane
      - helm upgrade --install -n istio-ingress --create-namespace istio-ingress istio/gateway --wait

  istio:deploy:self:minimal:
    cmds:
      # Install the control plane
      - helm repo add istio https://istio-release.storage.googleapis.com/charts
      - helm repo update
      - helm upgrade --install -n istio-system istiod istio/istiod --wait
      - kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml

  istio:deploy:self:ambient:
    cmds:
      # Install the control plane
      - helm repo add istio https://istio-release.storage.googleapis.com/charts
      - helm repo update
      - helm upgrade --install -n istio-system --create-namespace istio-base istio/base --wait
      - kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml
      - helm upgrade --install -n istio-system istiod istio/istiod --set profile=ambient --wait
      - helm upgrade --install -n kube-system istio-cni istio/cni -n kube-system
      # Install the data plane
      - helm upgrade --install ztunnel istio/ztunnel --wait
      # - helm upgrade --install -n istio-ingress --create-namespace istio-ingress istio/gateway --wait
      # - helm upgrade --install -n istio-system istio-egress istio/egressgateway

  istio:deploy:self:istioctl:
    cmds:
      # - istioctl install --set profile=ambient --skip-confirmation
      - istioctl install --set profile=default --skip-confirmation
      - istioctl verify-install

  juicefs:csi:deploy:self:
    cmds:
      - helm repo add juicefs https://juicedata.github.io/charts/
      - helm repo update
      - >
        helm upgrade --install
        juicefs-csi-driver juicefs/juicefs-csi-driver
        --namespace kube-system --create-namespace
        --reuse-values
        --set metrics.enabled=true
        --set controller.replicas=1
        {{.CLI_ARGS}}

  juicefs:csi:deploy:storage-class:
    cmds:
      - |-
        helm upgrade --install juicefs-csi-driver juicefs/juicefs-csi-driver --namespace kube-system --create-namespace --reuse-values --values - <<'EOF'
        # For production environment, manually create & manage storageClass outside Helm is recommended, ref: https://juicefs.com/docs/csi/guide/pv#create-storage-class
        storageClasses:
        - name: "juicefs-sc"
          # Set to true to actually create this StorageClass
          enabled: true
          # Either Retain or Delete, ref: https://juicefs.com/docs/csi/guide/resource-optimization#reclaim-policy
          reclaimPolicy: Retain
          # Set to true to allow PVC expansion
          allowVolumeExpansion: true
          # Additional annotations for this StorageClass, e.g. make it default
          # annotations:
          #   storageclass.kubernetes.io/is-default-class: "true"

          backend:
            # The JuiceFS file system name
            name: ""
            # Connection URL for metadata engine (e.g. Redis), for community edition use only, ref: https://juicefs.com/docs/community/databases_for_metadata
            metaurl: ""
            # Object storage type, such as s3, gs, oss, for community edition use only, ref: https://juicefs.com/docs/community/how_to_setup_object_storage
            storage: ""
            # Bucket URL, for community edition use only, ref: https://juicefs.com/docs/community/how_to_setup_object_storage
            bucket: ""
            # Token for JuiceFS Enterprise Edition token, ref: https://juicefs.com/docs/cloud/acl
            token: ""
            # Access key for object storage
            accessKey: ""
            # Secret key for object storage
            secretKey: ""
            # Environment variables for the JuiceFS Client
            # Example: {"a": "b"}
            # Ref: https://juicefs.com/docs/csi/guide/pv#volume-credentials
            envs: ""
            # Extra files for the mount pod, ref: https://juicefs.com/docs/csi/guide/pv/#mount-pod-extra-files
            configs: ""
            # The number of days which files are kept in the trash, for community edition use only, ref: https://juicefs.com/docs/community/security/trash
            trashDays: ""
            # Options passed to the "juicefs format" or "juicefs auth" command, depending on which edition you're using
            # Example: block-size=4096,capacity=10
            # Ref: https://juicefs.com/docs/community/command_reference#format and https://juicefs.com/docs/cloud/reference/commands_reference#auth
            formatOptions: ""

          # Options for the "juicefs mount" command
          # Example:
          # - debug
          # - cache-size=2048
          # - cache-dir=/var/foo
          # Ref: https://juicefs.com/docs/community/command_reference#mount and https://juicefs.com/docs/cloud/reference/commands_reference#mount
          mountOptions:
          # Customize PV directory format, ref: https://juicefs.com/docs/csi/guide/pv#using-path-pattern
          # If enabled, controller.provisioner must be set to true
          # Example: "${.PVC.namespace}-${.PVC.name}"
          pathPattern: ""

          # Using PVC as JuiceFS cache path, ref: https://juicefs.com/docs/csi/guide/cache#use-pvc-as-cache-path
          cachePVC: ""

          mountPod:
            # Mount pod resource requests & limits
            resources:
              limits:
                cpu: 5000m
                memory: 5Gi
              requests:
                cpu: 100m
                memory: 256Mi
            # Override mount pod image, ref: https://juicefs.com/docs/csi/guide/custom-image
            image: ""
            # Set annotations for the mount pod
            annotations: {}
        EOF

  juicefs:csi:format:
    desc: >-
      Create secret of JuiceFS
      <AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      BUCKET_URL
      DATABASE>
      [SECRET
      FS_NAME
      STORAGE
      ENCRYPT_KEY
      JFS_RSA_PASSPHRASE]
    requires:
      vars:
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - BUCKET_URL # https://<BUCKET>.s3.<REGION>.amazonaws.com
        - DATABASE
    vars:
      _SECRET: >-
        {{if .SECRET -}}
        {{.SECRET}}
        {{- else -}}
        juicefs-s3-secret
        {{- end}}
      _FS_NAME: >-
        {{if .FS_NAME -}}
        {{.FS_NAME}}
        {{- else -}}
        juicefs
        {{- end}}
      # file/s3/gs/oss/cos
      _STORAGE: >-
        {{if .STORAGE -}}
        {{.STORAGE}}
        {{- else -}}
        s3
        {{- end}}
      _TZ: >-
        {{if .TZ -}}
        {{.TZ}}
        {{- else -}}
        UTC
        {{- end}}
      # path/to/key.pem
      _ENCRYPT_RSA_KEY: >-
        {{if .ENCRYPT_RSA_KEY -}}
        ,--encrypt-rsa-key={{.ENCRYPT_KEY}}
        {{- end}}
    cmds:
      - |-
        kubectl apply -n default -f - <<'EOF'
        apiVersion: v1
        kind: Secret
        metadata:
          name: {{._SECRET}}
          namespace: default
          labels:
            juicefs.com/validate-secret: "true"
        type: Opaque
        stringData:
          name: {{._FS_NAME}}
          metaurl: {{.DATABASE}}
          storage: {{._STORAGE}}
          bucket: {{.BUCKET_URL}}
          access-key: {{.AWS_ACCESS_KEY_ID}}
          secret-key: {{.AWS_SECRET_ACCESS_KEY}}
          envs: "{TZ: {{._TZ}}{{if .JFS_RSA_PASSPHRASE}}, JFS_RSA_PASSPHRASE: {{.JFS_RSA_PASSPHRASE}}{{end}}}"
          format-options: --block-size=2M,--compress=lz4,trash-days=3{{._ENCRYPT_RSA_KEY}}
        EOF

  kadalu:deploy:self:
    vars:
      _VERSION: '1.3.0'
    cmds:
      - >
        helm upgrade --install
        kadalu https://github.com/kadalu/kadalu/releases/download/1.3.0/kadalu-helm-chart.tgz
        --namespace kadalu --create-namespace
        --set kubernetesDistro=kubernetes
        --set operator.enabled=true
        {{.CLI_ARGS}}

  kanister:deploy:self:
    cmds:
      - helm repo add kanister https://charts.kanister.io/
      - helm repo update
      - >
        helm upgrade --install
        kanister kanister/kanister-operator
        --namespace kanister --create-namespace

  k3sup-init-master:
    desc: 'Initial k3s cluster via SSH [SSH_HOST SSH_PORT SSH_USER SSH_KEY IS_EMBEDDED_ETCD IS_MERGE_EXISTING]'
    cmds:
      - cmd: >
          k3sup install
          {{if not .IS_EMBEDDED_ETCD}} {{else}} --cluster {{end}}
          {{if not .IS_MERGE_EXISTING}} {{else}} --merge {{end}}
          --host {{if .SSH_HOST}} {{.SSH_HOST}} {{else}} 127.0.0.1 {{end}}
          --ssh-port {{if .SSH_PORT}} {{.SSH_PORT}} {{else}} 22 {{end}}
          --user {{if .SSH_USER}} {{.SSH_USER}} {{else}} {{.USER}} {{end}}
          {{if .SSH_KEY}} --ssh-key {{.SSH_KEY}} {{end}}
          --k3s-channel stable

  k3sup-join-worker:
    desc: 'Join a k3s cluster via SSH <SSH_HOST SSH_USER MASTER_SSH_HOST MASTER_SSH_PORT>'
    cmds:
      - cmd: >
          k3sup join
          --server
          --host {{.SSH_HOST}}
          --user {{.SSH_USER}}
          --server-host {{.MASTER_SSH_HOST}}
          --server-user {{.MASTER_SSH_PORT}}
          --k3s-channel stable

  karpor:deploy:self:
    desc: 'Deploy karpor [STORAGE_CLASS]'
    vars:
      _STORAGE_CLASS: >-
        {{if .STORAGE_CLASS -}}
        --set etcd.persistence.storageClass={{.STORAGE_CLASS}}
        {{- end}}
    cmds:
      - helm repo add kusionstack https://kusionstack.github.io/charts --force-update
      - >-
        helm upgrade --install
        karpor kusionstack/karpor
        --reuse-values
        {{._STORAGE_CLASS}}
        --set server.resources.requests.cpu=100m
        --set syncer.resources.requests.cpu=100m
        --set elasticsearch.resources.requests.cpu=100m
        --set etcd.resources.requests.cpu=100m
        {{.CLI_ARGS}}

  karpor:get:config:
    cmds:
      - >-
        kubectl get configmap karpor-kubeconfig -n karpor -o go-template='{{.data.config}}'
        > $HOME/.kube/karpor-hub-cluster.kubeconfig

  karpo:create:sa:admin:
    env:
      KUBECONFIG: '$HOME/.kube/karpor-hub-cluster.kubeconfig'
    cmds:
      - kubectl create serviceaccount karpor-admin
      - kubectl create clusterrolebinding karpor-admin --clusterrole=karpor-admin --serviceaccount=default:karpor-admin

  karpo:create:sa:guest:
    env:
      KUBECONFIG: '$HOME/.kube/karpor-hub-cluster.kubeconfig'
    cmds:
      - kubectl create serviceaccount karpor-guest
      - kubectl create clusterrolebinding karpor-guest --clusterrole=karpor-guest --serviceaccount=default:karpor-guest

  karpo:create:token:admin:
    env:
      KUBECONFIG: '$HOME/.kube/karpor-hub-cluster.kubeconfig'
    cmds:
      - kubectl create token karpor-admin --duration=1000h

  karpo:create:token:guest:
    env:
      KUBECONFIG: '$HOME/.kube/karpor-hub-cluster.kubeconfig'
    cmds:
      - kubectl create token karpor-admin --duration=8760h

  kong:deploy:self:
    cmds:
      - >-
        helm upgrade --install
        kong --repo https://charts.konghq.com ingress
        --namespace kong --create-namespace
        --reuse-values
        {{.CLI_ARGS}}

  krew:install:plugin:
    desc: 'Install a kubectl plugin [PLUGIN] -- [plugin]'
    vars:
      _PLUGIN: '{{if .PLUGIN}}{{.PLUGIN}}{{else if .CLI_ARGS}}{{.CLI_ARGS}}{{else}}--help{{end}}'
    cmds:
      - kubectl krew install {{._PLUGIN}}

  krew:install:plugins:
    internal: true
    requires:
      vars: [PLUGINS]
    cmds:
      - task: krew:install:plugin
        vars:
          PLUGIN: '{{.PLUGIN}}'
        for: {var: PLUGINS, as: PLUGIN}

  krew:install:plugin:curated-list:
    desc: 'Install recommended plugins'
    cmds:
      - task: krew:install:plugins
        vars:
          PLUGINS:
            - access-matrix
            - cert-manager
            - cilium
            - cnpg
            - cost
            - explore
            - flame
            - kc
            - tree

  krew:install:plugin:curated-list:unix:
    platforms: [linux, darwin]
    desc: 'Install recommended plugins'
    cmds:
      - task: krew:install:plugins
        vars:
          PLUGINS:
            - ctx
            - ns
            - node-shell

  kube-vip:deploy:self:
    desc: 'Deploy kube-vip'
    cmds:
      - kubectl apply -f https://kube-vip.io/manifests/rbac.yaml
      - 'echo "TODO: DaemonSet"'
      - echo "To simplify, is is recommended to use loadBalancer to replace kube-vip"
      - echo "k3d comes with HA implemented by Nginx"

  kubero:deploy:self:full:
    cmds:
      - task: kubero:deploy:operator:full
      - task: kubero:deploy:ui

  kubero:deploy:self:minimal:
    cmds:
      - task: kubero:deploy:operator:minimal
      - task: kubero:deploy:ui

  kubero:deploy:operator:minimal:
    cmds:
      - kubectl apply -f https://raw.githubusercontent.com/kubero-dev/kubero-operator/main/deploy/operator.yaml

  kubero:deploy:operator:full:
    cmds:
      - kubectl create -f https://raw.githubusercontent.com/operator-framework/operator-lifecycle-manager/master/deploy/upstream/quickstart/crds.yaml
      - kubectl create -f https://raw.githubusercontent.com/operator-framework/operator-lifecycle-manager/master/deploy/upstream/quickstart/olm.yaml
      - kubectl create -f https://operatorhub.io/install/kubero-operator.yaml

  kubero:deploy:ui:
    requires:
      vars:
        - 'KUBERO_WEBHOOK_SECRET' # openssl rand -hex 20
        - 'KUBERO_SESSION_KEY' # openssl rand -hex 20
    cmds:
      - kubectl create namespace kubero
      - >
        kubectl create secret generic kubero-secrets
        -n kubero
        --from-literal=KUBERO_WEBHOOK_SECRET={{.KUBERO_WEBHOOK_SECRET}}
        --from-literal=KUBERO_SESSION_KEY={{.KUBERO_SESSION_KEY}}
        {{.CLI_ARGS}}
      - >
        kubectl apply
        -f https://raw.githubusercontent.com/kubero-dev/kubero-operator/main/config/samples/application_v1alpha1_kubero.yaml
        -n kubero

  kubeshark:deploy:self:
    cmds:
      - helm repo add kubeshark https://helm.kubeshark.co
      - helm repo update
      - >
        helm upgrade --install
        kubeshark kubeshark/kubeshark
        --namespace kubeshark --create-namespace

  kubesphere:deploy:self:
    desc: 'Deploy KubeSphere. http://ip:30880 with "admin" "P@88w0rd"'
    cmds:
      - >
        helm upgrade --install
        ks-core https://charts.kubesphere.io/main/ks-core-1.1.2.tgz
        -n kubesphere-system --create-namespace
        --debug
        --wait
        {{.CLI_ARGS}}

  kubevela:deploy:self:
    cmds:
      - helm repo add kubevela https://kubevela.github.io/charts
      - helm repo update
      - >
        helm upgrade --install
        kubevela kubevela/vela-core
        -n vela-system --create-namespace
        --wait

  local-path-provisioner:deploy:self:k3s:
    cmds:
      - task: local-path-provisioner:deploy:self
        vars:
          K3S: "true"

  local-path-provisioner:deploy:self:
    silent: true
    vars:
      _DATA_DIR: >-
        {{if .DATA_DIR -}}
        {{.DATA_DIR}}
        {{- else -}}
        /opt/local-path-provisioner
        {{- end}}
    cmds:
      # - kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml
      - |-
        kubectl apply -f - <<'EOF'
        apiVersion: v1
        kind: Namespace
        metadata:
          name: local-path-storage

        ---
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: local-path-provisioner-service-account
          namespace: local-path-storage

        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          name: local-path-provisioner-role
          namespace: local-path-storage
        rules:
          - apiGroups: [""]
            resources: ["pods"]
            verbs: ["get", "list", "watch", "create", "patch", "update", "delete"]

        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          name: local-path-provisioner-role
        rules:
          - apiGroups: [""]
            resources: ["nodes", "persistentvolumeclaims", "configmaps", "pods", "pods/log"]
            verbs: ["get", "list", "watch"]
          - apiGroups: [""]
            resources: ["persistentvolumes"]
            verbs: ["get", "list", "watch", "create", "patch", "update", "delete"]
          - apiGroups: [""]
            resources: ["events"]
            verbs: ["create", "patch"]
          - apiGroups: ["storage.k8s.io"]
            resources: ["storageclasses"]
            verbs: ["get", "list", "watch"]

        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: local-path-provisioner-bind
          namespace: local-path-storage
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: local-path-provisioner-role
        subjects:
          - kind: ServiceAccount
            name: local-path-provisioner-service-account
            namespace: local-path-storage

        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: local-path-provisioner-bind
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: local-path-provisioner-role
        subjects:
          - kind: ServiceAccount
            name: local-path-provisioner-service-account
            namespace: local-path-storage

        ---
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: local-path-provisioner
          namespace: local-path-storage
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: local-path-provisioner
          template:
            metadata:
              labels:
                app: local-path-provisioner
            spec:
              serviceAccountName: local-path-provisioner-service-account
              containers:
                - name: local-path-provisioner
                  image: rancher/local-path-provisioner:v0.0.30
                  imagePullPolicy: IfNotPresent
                  command:
                    - local-path-provisioner
                    - --debug
                    - start
                    - --config
                    - /etc/config/config.json
                  volumeMounts:
                    - name: config-volume
                      mountPath: /etc/config/
                  env:
                    - name: POD_NAMESPACE
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                    - name: CONFIG_MOUNT_PATH
                      value: /etc/config/
              volumes:
                - name: config-volume
                  configMap:
                    name: local-path-config

        ---
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
          name: local-path
        provisioner: rancher.io/local-path
        volumeBindingMode: WaitForFirstConsumer
        reclaimPolicy: Delete

        ---
        kind: ConfigMap
        apiVersion: v1
        metadata:
          name: local-path-config
          namespace: local-path-storage
        data:
          config.json: |-
            {
                    "nodePathMap":[
                    {
                            "node":"DEFAULT_PATH_FOR_NON_LISTED_NODES",
                            "paths":["{{._DATA_DIR}}"]
                    }
                    ]
            }
          setup: |-
            #!/bin/sh
            set -eu
            mkdir -m 0777 -p "$VOL_DIR"
          teardown: |-
            #!/bin/sh
            set -eu
            rm -rf "$VOL_DIR"
          helperPod.yaml: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: helper-pod
            spec:
              priorityClassName: system-node-critical
              tolerations:
                - key: node.kubernetes.io/disk-pressure
                  operator: Exists
                  effect: NoSchedule
              containers:
              - name: helper-pod
                image: busybox
                imagePullPolicy: IfNotPresent
        EOF

  kmesh:deploy:self:
    desc: 'Deploy kmesh. Please install istio ahead'
    dir: '{{.USER_WORKING_DIR}}'
    cmds:
      - git clone https://github.com/kmesh-net/kmesh
      - >-
        helm upgrade install
        kmesh ./kmesh/deploy/charts/kmesh-helm
        -n kmesh-system --create-namespace
        --reuse-values
        {{.CLI_ARGS}}
      - rm -r kmesh

  kuma:deploy:self:
    desc: >-
      Deploy kuma
      [
      INGRESS
      INGRESS_SERVICE_TYPE
      EGRESS
      EGRESS_SERVICE_TYPE
      ]
      --
      [OPTIONS]
    vars:
      _RESOURCES: >-
        {{if ne .RESOURCES "no" -}}
        --set controlPlane.resources.requests.cpu=200m
        {{- end}}
      _INGRESS: >-
        {{if ne .INGRESS "no" -}}
        --set ingress.enabled=true
        --set ingress.service.type={{default "LoadBalancer" .INGRESS_SERVICE_TYPE}}
        {{- end}}
      _EGRESS: >-
        {{if eq .EGRESS "yes" -}}
        --set egress.enabled=true
        --set egress.service.type={{default "ClientIP" .EGRESS_SERVICE_TYPE}}
        {{- end}}
    cmds:
      - helm repo add --force-update kuma https://kumahq.github.io/charts
      - >-
        helm upgrade --install
        --create-namespace --namespace kuma-system
        kuma kuma/kuma
        --reuse-values
        {{._RESOURCES}}
        {{._INGRESS}}
        {{._EGRESS}}
        {{.CLI_ARGS}}

  manifest:validate:
    desc: 'Validate manifests -- <MANIFEST>...'
    dir: '{{.USER_WORKING_DIR}}'
    cmds:
      - >
        kubeconform
        -schema-location default
        -schema-location 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{"{{"}}.Group{{"}}"}}/{{"{{"}}.ResourceKind{{"}}"}}_{{"{{"}}.ResourceAPIVersion{{"}}"}}.json'
        {{.CLI_ARGS}}

  multus:deploy:self:k3s:whereabouts:
    desc: 'Deploy multus CNI [DATA_DIR]'
    vars:
      _DATA_DIR: '{{if .DATA_DIR}}{{.DATA_DIR}}{{else}}/var/lib/rancher/k3s{{end}}'
    cmds:
      - helm repo add rke2-charts https://rke2-charts.rancher.io
      - helm repo update
      - |-
        helm upgrade --install multus rke2-charts/rke2-multus -n kube-system --values - <<'EOF'
        config:
          cni_conf:
            confDir: {{._DATA_DIR}}/agent/etc/cni/net.d
            binDir: {{._DATA_DIR}}/data/current/bin/
            kubeconfig: {{._DATA_DIR}}/agent/etc/cni/net.d/multus.d/multus.kubeconfig
        rke2-whereabouts:
          fullnameOverride: whereabouts
          enabled: true
          cniConf:
            confDir: {{._DATA_DIR}}/agent/etc/cni/net.d
            binDir: {{._DATA_DIR}}/data/current/bin/
        EOF

  longhorn:prepare:deps:iscsi:
    platforms: [linux]
    cmds:
      - kubectl apply --wait -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.1/deploy/prerequisite/longhorn-iscsi-installation.yaml

  longhorn:prepare:deps:nfs:
    platforms: [linux]
    cmds:
      - kubectl apply --wait -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.1/deploy/prerequisite/longhorn-nfs-installation.yaml

  longhorn:prepare:deps:cryptsetup:
    platforms: [linux]
    desc: 'Optional'
    vars:
      OS:
        sh: >-
          OS=$(grep -E "^ID_LIKE=" /etc/os-release | cut -d '=' -f 2);
          if [[ -z "${OS}" ]];
          then OS=$(grep -E "^ID=" /etc/os-release | cut -d '=' -f 2);
          fi;
          echo ${OS};
      DEBIAN:
        sh: >-
          if [[ "{{.OS}}" == *"debian"* ]]; then echo "yes"; else echo "no"; fi
      FEDORA:
        sh: >-
          if [[ "{{.OS}}" == *"fedora"* ]]; then echo "yes"; else echo "no"; fi
      SUSE:
        sh: >-
          if [[ "{{.OS}}" == *"suse"* ]]; then echo "yes"; else echo "no"; fi
    cmds:
      - >-
        {{if eq .DEBIAN "yes" -}}
        sudo apt-get install -y cryptsetup dmsetup
        {{- end}}
      - >-
        {{if eq .FEDORA "yes" -}}
        sudo yum install -y cryptsetup device-mapper
        {{- end}}
      - >-
        {{if eq .SUSE "yes" -}}
        sudo zypper install -y cryptsetup device-mapper
        {{- end}}

  longhorn:prepare:check:
    platforms: [linux]
    preconditions:
      - kubectl version
      - jq --version
      - mktemp --version
      - sort --version
      - printf "Found printf"
    requires:
      vars:
        - KUBECONFIG
    cmds:
      - export KUBECONFIG={{.KUBECONFIG}}; curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.7.1/scripts/environment_check.sh | bash

  longhorn:deploy:self:
    vars:
      _VERSION: '1.7.1'
      _RELEASE: 'longhorn'
      _DATA_DIR: >-
        {{if .DATA_DIR -}}
        {{.DATA_DIR}}
        {{- else -}}
        /var/lib/longhorn/
        {{- end}}
    cmds:
      - helm repo add longhorn https://charts.longhorn.io
      - helm repo update
      - >
        helm upgrade --install
        {{._RELEASE}} longhorn/longhorn
        --version {{._VERSION}}
        --namespace longhorn-system --create-namespace
        --reuse-values
        --set defaultSettings.defaultDataPath={{._DATA_DIR}}
        --set defaultSettings.storageMinimalAvailablePercentage=12
        --set defaultSettings.defaultReplicaCount=1
        --set defaultSettings.backupTarget=s3://longhorn@us-east-1/
        --set longhornUI.replicas=1
        --set persistence.migratable=true
        --set csi.attacherReplicaCount=1
        --set csi.provisionerReplicaCount=1
        --set csi.resizerReplicaCount=1
        --set csi.snapshotterReplicaCount=1
        {{.CLI_ARGS}}

  longhorn:backup:s3:
    desc: >-
      Create secret of longhorn backup target
      <AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_ENDPOINTS>
      [VIRTUAL_HOSTED_STYLE]
    requires:
      vars:
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - AWS_ENDPOINTS # http(s)://host:port
    vars:
      _SECRET: >-
        {{if .SECRET -}}
        {{.SECRET}}
        {{- else -}}
        longhorn-backup-s3-compatible-secret
        {{- end}}
      _VIRTUAL_HOSTED_STYLE: >-
        {{if eq .VIRTUAL_HOSTED_STYLE "false" -}}
        false
        {{- else -}}
        true
        {{- end}}
    cmds:
      - >-
        kubectl create secret generic {{._SECRET}}
        -n longhorn-system
        --type Opaque
        --from-literal AWS_ACCESS_KEY_ID={{.AWS_ACCESS_KEY_ID}}
        --from-literal AWS_SECRET_ACCESS_KEY={{.AWS_SECRET_ACCESS_KEY}}
        --from-literal AWS_ENDPOINTS={{.AWS_ENDPOINTS}}
        --from-literal VIRTUAL_HOSTED_STYLE={{._VIRTUAL_HOSTED_STYLE}}
        {{.CLI_ARGS}}

  longhorn:backup:s3:update:
    desc: >-
      Update secret of longhorn backup target
      <AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_ENDPOINTS>
      [VIRTUAL_HOSTED_STYLE]
    requires:
      vars:
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - AWS_ENDPOINTS # http(s)://host:port
    vars:
      _SECRET: >-
        {{if .SECRET -}}
        {{.SECRET}}
        {{- else -}}
        longhorn-backup-s3-compatible-secret
        {{- end}}
      AWS_ACCESS_KEY_ID_BASE64:
        sh: 'echo {{.AWS_ACCESS_KEY_ID}} | base64'
      AWS_SECRET_ACCESS_KEY_BASE64:
        sh: 'echo {{.AWS_SECRET_ACCESS_KEY}} | base64'
      AWS_ENDPOINTS_BASE64:
        sh: 'echo {{.AWS_ENDPOINTS}} | base64'
      VIRTUAL_HOSTED_STYLE_BASE64: >-
        {{if eq .VIRTUAL_HOSTED_STYLE "false" -}}
        ZmFsc2U=
        {{- else -}}
        dHJ1ZQ==
        {{- end}}
    cmds:
      - |-
        kubectl apply -f - <<'EOF'
        apiVersion: v1
        kind: Secret
        metadata:
          name: {{._SECRET}}
          namespace: longhorn-system
        type: Opaque
        data:
          AWS_ACCESS_KEY_ID: {{.AWS_ACCESS_KEY_ID_BASE64}}
          AWS_SECRET_ACCESS_KEY: '{{.AWS_SECRET_ACCESS_KEY_BASE64}}'
          AWS_ENDPOINTS: '{{.AWS_ENDPOINTS_BASE64}}'
          VIRTUAL_HOSTED_STYLE: '{{.VIRTUAL_HOSTED_STYLE_BASE64}}'
        EOF

  metallb:deploy:self:
    cmds:
      - helm repo add --force-update metallb https://metallb.github.io/metallb
      - >-
        helm upgrade --install
        metallb metallb/metallb
        -n metallb-system --create-namespace
        {{.CLI_ARGS}}

  metallb:deploy:self:bitnami:
    cmds:
      - >-
        helm upgrade --install
        metallb oci://registry-1.docker.io/bitnamicharts/metallb
        -n metallb-system --create-namespace
        {{.CLI_ARGS}}

  metallb:deploy:pool:
    vars:
      _NAME: '{{default "metallb-pool" .NAME}}'
      _NAMESPACE: '{{default "metallb-system" .NAMESPACE}}'
      _CIDR: '{{default "192.168.10.0/24" .CIDR}}'
    cmds:
      - |-
        kubectl apply -f - <<'EOF'
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          # A name for the address pool. Services can request allocation
          # from a specific address pool using this name.
          name: {{._NAME}}
          namespace: {{._NAMESPACE}}
        spec:
          # A list of IP address ranges over which MetalLB has
          # authority. You can list multiple ranges in a single pool, they
          # will all share the same settings. Each range can be either a
          # CIDR prefix, or an explicit start-end range of IPs.
          addresses:
          - {{._CIDR}}
          # - 192.168.9.1-192.168.9.5
          # - 192.168.10.0/24
          # - fc00:f853:0ccd:e799::/124
        EOF

  metallb:deploy:ad:
    vars:
      _NAME: '{{default "metallb-ad" .NAME}}'
      _NAMESPACE: '{{default "metallb-system" .NAMESPACE}}'
      _POOL: '{{default "metallb-pool" .POOL}}'
    cmds:
      - |-
        kubectl apply -f - <<'EOF'
        kind: L2Advertisement
        apiVersion: metallb.io/v1beta1
        metadata:
          name: {{._NAME}}
          namespace: {{._NAMESPACE}}
        spec:
          ipAddressPools: [{{._POOL}}]
        EOF

  nginx:deploy:self:
    cmds:
      - >
        helm upgrade --install
        ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx
        --namespace ingress-nginx --create-namespace

  nuclio:deploy:self:
    cmds:
      - helm repo add nuclio https://nuclio.github.io/nuclio/charts --force-update
      - >-
        helm upgrade --install
        nuclio nuclio/nuclio
        --namespace nuclio --create-namespace
        --reuse-values
        {{.CLI_ARGS}}

  openfaas:deploy:self:
    cmds:
      - kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml
      - helm repo add openfaas https://openfaas.github.io/faas-netes/
      - helm repo update
      - >
        helm upgrade --install
        openfaas openfaas/openfaas
        --namespace openfaas --create-namespace

  prometheus:deploy:community:cnpg:
    cmds:
      - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
      - helm repo update
      - >
        helm upgrade --install
        prometheus-community prometheus-community/kube-prometheus-stack
        --values https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/main/docs/src/samples/monitoring/kube-stack-config.yaml

  rainbond:deploy:self:
    desc: >-
      [
      K3S
      CONTAINERD
      ]
      -- [helm_upgrade_options]
    vars:
      _K3S: >-
        {{if eq .K3S "yes" -}}
        --set useK3sContainerd=true
        {{- end}}
      _CONTAINERD: >-
        {{if and (eq .CONTAINERD "yes") (ne .K3S "yes") -}}
        --set operator.env[0].name=CONTAINER_RUNTIME
        --set operator.env[0].value=containerd
        {{- end}}
    cmds:
      - helm repo add rainbond https://openchart.goodrain.com/goodrain/rainbond --force-update
      - >-
        helm upgrade --install
        rainbond rainbond/rainbond-cluster
        -n rbd-system --create-namespace
        --reuse-values
        {{._K3S}}
        {{._CONTAINERD}}
        {{.CLI_ARGS}}

  rainbond:deploy:self:k3s:
    cmds:
      - task: rainbond:deploy:self
        vars:
          K3S: "yes"

  rancher:deploy:self:
    vars:
      _IP: '{{default "127.0.0.1" .IP}}'
    cmds:
      - helm repo add rancher-latest https://releases.rancher.com/server-charts/latest --force-update
      - >-
        helm upgrade --install
        rancher rancher-latest/rancher
        --namespace cattle-system --create-namespace
        --reuse-values
        --set replicas=1
        --set hostname={{._IP}}.sslip.io

  rclone:csi:deploy:self:
    vars:
      _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}1.20{{end}}'
    cmds:
      - git clone https://github.com/wunderio/csi-rclone
      - kubectl apply -f csi-rclone/deploy/kubernetes/{{._VERSION}}/
      - rm -r csi-rclone
      - kubectl delete storageclass rclone
      - |-
        kubectl apply -f - <<'EOF'
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
          name: rclone
        provisioner: csi-rclone
        parameters:
          pathPattern: "${.PVC.namespace}/${.PVC.annotations.csi-rclone/storage-path}"
        EOF

  rclone:csi:deploy:remote:default:s3:r2:
    cmds:
      - task: rclone:csi:deploy:remote:default:s3
        vars:
          PROVIDER: Cloudflare

  rclone:csi:deploy:remote:default:s3:
    requires:
      vars:
        - S3_ENDPOINT # http(s)://host:port
        - ACCESS_KEY_ID
        - SECRET_ACCESS_KEY
    vars:
      _PROVIDER: '{{if .PROVIDER}}{{.PROVIDER}}{{else}}Other{{end}}'
      _DIR: '{{if .DIR}}{{.DIR}}{{else}}{{end}}'
    cmds:
      - |-
        kubectl apply -n csi-rclone -f - <<'EOF'
        apiVersion: v1
        kind: Secret
        metadata:
          name: rclone-secret
        type: Opaque
        stringData:
          remote: "default"
          remotePath: "projectname"
          configData: |
            [default]
            type = s3
            provider = {{._PROVIDER}}
            access_key_id = {{.ACCESS_KEY_ID}}
            secret_access_key = {{.SECRET_ACCESS_KEY}}
            endpoint = {{.S3_ENDPOINT}}
            encoding = Slash
        EOF

  rook:deploy:self:
    cmds:
      - task: rook:deploy:operator
      - task: rook:deploy:cluster

  rook:deploy:operator:
    cmds:
      - helm repo add rook-release https://charts.rook.io/release
      - helm repo update
      - >-
        helm upgrade --install
        --create-namespace --namespace rook-ceph
        rook-ceph rook-release/rook-ceph
        --wait
        --set resources.requests.cpu=100m

  rook:deploy:cluster:
    cmds:
      - helm repo add rook-release https://charts.rook.io/release
      - helm repo update
      - >-
        helm upgrade --install
        --create-namespace --namespace rook-ceph
        rook-ceph-cluster
        rook-release/rook-ceph-cluster
        --set operatorNamespace=rook-ceph

  traefik:deploy:self:
    desc: 'Deploy traefik [RELEASE VERSION CRD INGRESS GWAPI] -- [options]'
    vars:
      _RELEASE: >
        {{if .RELEASE -}}
        {{.RELEASE}}
        {{- else -}}
        traefik
        {{- end}}
      _VERSION: >
        {{if .VERSION -}}
        {{.VERSION}}
        {{- else -}}
        32.0.0
        {{- end}}
      _NAMESPACE: '--namespace {{if .NAMESPACE}}{{.NAMESPACE}}{{else}}traefik{{end}} --create-namespace'
      _METRICS: >
        {{if true -}}
        --set metrics.prometheus.enabled=true
        --set metrics.prometheus.service.enabled=true
        {{- end}}
      _DAEMONSET: '{{if ne .DAEMONSET "false"}}--set deployment.kind=DaemonSet{{end}}'
      _CRD: >
        {{if eq .CRD "false" -}}
        --set providers.kubernetesCRD.enabled=false
        {{- else -}}
        --set providers.kubernetesCRD.allowCrossNamespace=true
        --set providers.kubernetesCRD.allowExternalNameServices=true
        --set ingressRoute.dashboard.enabled=true
        {{- end}}
      _INGRESS: >
        {{if eq .INGRESS "false" -}}
        --set providers.kubernetesIngress.enabled=false
        {{- else -}}
        --set providers.kubernetesIngress.allowExternalNameServices=true
        {{if ne .INGRESS_DEFAULT "true"}}--set ingressClass.isDefaultClass=false{{end}}
        {{- end}}
      _GWAPI: >
        {{if ne .GWAPI "false" -}}
        --set providers.kubernetesGateway.enabled=true
        --set providers.kubernetesGateway.experimentalChannel=true
        {{- end}}
    cmds:
      - helm repo add traefik https://traefik.github.io/charts
      - helm repo update
      - >
        helm upgrade --install
        {{._RELEASE}} traefik/traefik --version {{._VERSION}}
        {{._NAMESPACE}}
        {{._METRICS}}
        {{._DAEMONSET}}
        {{._CRD}}
        {{._INGRESS}}
        {{._GWAPI}}
        {{.CLI_ARGS}}

  tsuru:deploy:self:
    cmds:
      - helm repo add tsuru https://tsuru.github.io/charts
      - helm repo update
      - >
        helm upgrade --install
        tsuru tsuru/tsuru-stack
        --create-namespace --namespace tsuru-system
        --set ingress-nginx.enabled=false
        --set tsuru-api.service.type=ClusterIP

  velero:deploy:self:
    desc: 'Deploy velero 1.14.1'
    cmds:
      - helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts
      - helm repo update
      - >
        helm upgrade --install
        velero vmware-tanzu/velero
        --version 7.2.1
        {{.CLI_ARGS}}
