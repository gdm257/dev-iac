version: '3'

tasks:

  anteon:deploy:self:
    cmds:
      - helm repo add anteon https://getanteon.github.io/anteon-helm-charts/
      - helm repo update
      # Anteon platform
      - >
        helm upgrade --install
        anteon-selfhosted anteon/anteon
        --namespace anteon --create-namespace
        --wait
        --reuse-values

  anteon:deploy:alaz:
    requires:
      vars: [MONITORING_ID]
    cmds:
      - helm repo add anteon https://getanteon.github.io/anteon-helm-charts/
      - helm repo update
      # eBPF agent (Alaz)
      - >
        helm upgrade --install
        alaz anteon/alaz
        --namespace anteon --create-namespace
        --set monitoringID={{.MONITORING_ID}}

  apisix:deploy:self:
    cmds:
      - task: apisix:deploy:bitnami

  apisix:deploy:bitnami:
    vars:
      _INGRESS: >-
        {{if ne INGRESS "no" -}}
        --set ingressController.enabled=true
        {{- end}}
      _DASHBOARD: >-
        {{if ne DASHBOARD "no" -}}
        --set dashboard.enabled=true
        {{- end}}
    cmds:
      - helm repo add --force-update apisix https://charts.apiseven.com
      - >-
        helm upgrade --install
        apisix oci://registry-1.docker.io/bitnamicharts/apisix
        --version 0.14.0
        --create-namespace --namespace ingress-apisix
        --reuse-values
        {{._INGRESS}}
        {{._DASHBOARD}}
        {{.CLI_ARGS}}

  apisix:deploy:ingress:
    vars:
      _GATEWAY_API: >-
        {{if ne GATEWAY_API "no" -}}
        --set config.kubernetes.enableGatewayAPI=true
        {{- end}}
    cmds:
      - helm repo add --force-update apisix https://charts.apiseven.com
      - >-
        helm upgrade --install
        apisix-ingress-controller apisix/apisix-ingress-controller
        --version 0.14.0
        --create-namespace --namespace ingress-apisix
        --reuse-values
        {{._GATEWAY_API}}
        {{.CLI_ARGS}}

  apisix:deploy:normal:
    cmds:
      - helm repo add --force-update apisix https://charts.apiseven.com
      - >-
        helm upgrade --install
        apisix apisix/apisix
        --version 2.9.0
        --create-namespace  --namespace apisix
        --reuse-values
        {{.CLI_ARGS}}

  apisix:deploy:dashboard:
    cmds:
      - helm repo add --force-update apisix https://charts.apiseven.com
      - >-
        helm upgrade --install
        apisix-dashboard apisix/apisix-dashboard
        --version 0.8.2
        --create-namespace --namespace apisix
        --reuse-values
        {{.CLI_ARGS}}

  argocd:deploy:self:helm:
    desc: >-
      Deploy ArgoCD controllers to cluster
      [
      ENABLE_KUSTOMIZE_HELM
      ]
    vars:
      _ENABLE_HELM: >-
        {{if ne .ENABLE_KUSTOMIZE_HELM "false" -}}
        --set configs.cm.kustomize.buildOptions=--enable-helm
        {{- end}}
    cmds:
      - helm repo add argo https://argoproj.github.io/argo-helm --force-update
      - >-
        helm upgrade --install
        argocd argo/argo-cd
        --namespace argocd --create-namespace
        --reuse-values
        {{_ENABLE_HELM}}
        {{.CLI_ARGS}}

  argocd:deploy:self:kubectl:
    desc: 'Deploy ArgoCD controllers without HA to cluster [CONTEXT]'
    vars:
      _CONTEXT: '{{if .CONTEXT}}--context {{.CONTEXT}}{{end}}'
    cmds:
      - kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml {{._CONTEXT}}

  argocd:admin:get-initial-password:
    desc: 'Print initial admin password i.e. secret "argocd-initial-admin-secret" base64 encoded'
    cmds:
      - argocd admin initial-password -n argocd

  argocd:admin:passwd:
    desc: 'Change admin password'
    cmds:
      - argocd account update-password

  carina:deploy:self:
    vars:
      _VERSION: 'v0.11.0'
    cmds:
      - helm repo add carina-csi-driver https://carina-io.github.io
      - helm repo update
      - >
        helm upgrade --install
        carina-csi-driver carina-csi-driver/carina-csi-driver
        --version {{._VERSION}}
        --namespace kube-system --create-namespace

  cert-manager:deploy:self:
    vars:
      _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}v1.15.3{{end}}'
      _GATEWAY_API: >-
        {{if eq .GATEWAY_API "yes" -}}
        --set "extraArgs={--enable-gateway-api}"
        {{- end}}
    cmds:
      - helm repo add jetstack https://charts.jetstack.io --force-update
      - >
        helm upgrade --install
        cert-manager jetstack/cert-manager
        --version {{._VERSION}}
        --namespace cert-manager --create-namespace
        --set crds.enabled=true
        {{._GATEWAY_API}}

  cert-manager:deploy:csi:
    vars:
      _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}v0.10.1{{end}}'
    cmds:
      - helm repo add jetstack https://charts.jetstack.io --force-update
      - >
        helm upgrade --install
        cert-manager-csi-driver jetstack/cert-manager-csi-driver
        --version {{._VERSION}}
        --namespace cert-manager --create-namespace
        {{.CLI_ARGS}}

  cert-manager:deploy:issuer:letsencrypt:
    desc: >-
      [
      ISSUER_NAME
      IS_CLUSTER
      NAMESPACE

      IS_HTTP01
      INGRESS_CLASS

      IS_DNS01
      IS_LETSENCRYPT
      IS_CF
      EMAIL
      ACCOUNT_SECRET_NAME
      SECRET_NAME
      SECRET_KEY_NAME
      SECRET_KEY_VALUE

      DNS_NAMES
      DNS_ZONES
      ]
    vars:
      _ISSUER_KIND: >-
        {{if eq .IS_CLUSTER "yes" -}}
        ClusterIssuer
        {{- else -}}
        Issuer
        {{- end}}
      _ACME_SERVER: >-
        {{if eq .IS_STAGING "yes" -}}
        https://acme-staging-v02.api.letsencrypt.org/directory
        {{- else -}}
        https://acme-v02.api.letsencrypt.org/directory
        {{- end}}
      SECRET_KEY_VALUE_BASE64:
        sh: >-
          echo -n "{{.SECRET_KEY_VALUE}}" | base64
    cmds:
      - |-
        kubectl apply -f - <<EOF
        ---
        apiVersion: v1
        kind: Secret
        metadata:
          name: {{default "issuer-dns01-secret" .SECRET_NAME}}
          namespace: {{default "default" .NAMESPACE}}
        type: Opaque
        data:
          {{default "api-token" .SECRET_KEY_NAME}}: {{.SECRET_KEY_VALUE_BASE64}}
        ---
        apiVersion: cert-manager.io/v1
        kind: {{._ISSUER_KIND}}
        metadata:
          name: {{default "letsencrypt-issuer" .ISSUER_NAME}}
          namespace: {{default "default" .NAMESPACE}}
        spec:
          acme:
            email: {{.EMAIL}}
            server: {{._ACME_SERVER}}
            privateKeySecretRef:
              name: {{default "letsencrypt-account-key" .ACCOUNT_SECRET_NAME}}
            solvers:
            {{- if ne .IS_DNS01 "no"}}
            - dns01:
                {{- if ne .IS_CF "no"}}
                cloudflare:
                  apiTokenSecretRef:
                    name: {{default "issuer-dns01-secret" .SECRET_NAME}}
                    key: {{default "api-token" .SECRET_KEY_NAME}}
                {{- end}}
              {{- if or .DNS_NAMES .DNS_ZONES}}
              selector:
                dnsNames: [{{.DNS_NAMES}}]
              {{- end}}
            {{- end}}
            {{- if eq .IS_HTTP01 "yes"}}
            - http01:
                ingress:
                  ingressClassName: {{default "nginx" .INGRESS_CLASS}}
              {{- if or .DNS_NAMES .DNS_ZONES}}
              selector:
                {{- if or .DNS_NAMES}}
                dnsNames: [{{.DNS_NAMES}}]
                {{- end}}
                {{- if or .DNS_ZONES}}
                dnsZones: [{{.DNS_ZONES}}]
                {{- end}}
              {{- end}}
            {{- end}}
        EOF

  cert-manager:deploy:cert:
    desc: >-
      [
      CERT_NAME
      NAMESPACE
      ISSUER_NAME
      IS_CLUSTER
      DOMAINS
      IPS
      ALGORITHM
      ENCODING
      SIZE
      SECRET_NAME
      ]
    vars:
      _ISSUER_KIND: >-
        {{if eq .IS_CLUSTER "yes" -}}
        ClusterIssuer
        {{- else -}}
        Issuer
        {{- end}}
    cmds:
      - |-
        kubectl apply -f - <<EOF
        apiVersion: cert-manager.io/v1
        kind: Certificate
        metadata:
          name: {{default "letsencrypt-cert" .CERT_NAME}}
          namespace: {{default "default" .NAMESPACE}}
        spec:
          issuerRef:
            name: {{default "letsencrypt-issuer" .ISSUER_NAME}}
            kind: {{._ISSUER_KIND}}
          secretName: {{default "letsencrypt-cert-secret" .SECRET_NAME}}
          {{- if .DOMAINS}}
          dnsNames: [{{.DOMAINS}}]
          {{- end}}
          {{- if .IPS}}
          ipAddresses: [{{.IPS}}]
          {{- end}}
          privateKey:
            algorithm: {{default "RSA" .ALGORITHM}}
            encoding: {{default "PKCS1" .ENCODING}}
            size: {{default 2048 .SIZE}}
          renewBefore: 1440h
        EOF

  cert-manager:grant:
    requires:
      vars:
        - NAMESPACE
        - GATEWAY_NAMESPACE
    cmds:
      - |-
        kubectl apply -f - <<EOF
        apiVersion: gateway.networking.k8s.io/v1beta1
        kind: ReferenceGrant
        metadata:
          name: {{default "gateway-cert-rg" .NAME}}
          namespace: {{default "default" .NAMESPACE}}
        spec:
          from:
          - group: gateway.networking.k8s.io
            kind: Gateway
            namespace: {{default "default" .GATEWAY_NAMESPACE}}
          to:
          - kind: Secret
            group: ""
        EOF

  cilium:deploy:self:k3s:
    desc: >-
      Deploy cilium
      [
      POD_CIDR
      IS_ISTIO
      ]
      -- [options]
    vars:
      _VERSION: '{{default "1.16.2" .VERSION}}'
      # Default IPv4 Pod CIDR is 10.0.0.0/8
      # If your node network is in the same range,
      # you will lose connectivity to other nodes.
      _POD_CIDR: >-
        {{if .POD_CIDR -}}
        --set ipam.clusterPoolIPv4PodCIDRList[0]={{.POD_CIDR}}
        --set ipam.operator.clusterPoolIPv4PodCIDRList[0]={{.POD_CIDR}}
        {{- end}}
      _ISTIO: >-
        {{if ne .IS_ISTIO "no" -}}
        --set kubeProxyReplacement=true
        --set socketLB.hostNamespaceOnly=true
        {{- end}}
      _INGRESS: >-
        {{if eq .INGRESS "yes" -}}
        --set ingressController.enabled=true
        --set ingressController.loadbalancerMode=dedicated
        --set ingressController.default=true
        {{- end}}
      _EGRESS: >-
        {{if eq .EGRESS "yes" -}}
        --set egressGateway.enabled=true
        --set bpf.masquerade=true
        --set kubeProxyReplacement=true
        {{- end}}
      _GATEWAY_API: >-
        {{if eq .GATEWAY_API "yes" -}}
        --set kubeProxyReplacement=true
        --set gatewayAPI.enabled=true
        {{- end}}
      _NODE_PORT: >-
        {{if eq .NODE_PORT "yes" -}}
        --set nodePort.enabled=true
        {{- end}}
      _HOST_PORT: >-
        {{if eq .HOST_PORT "yes" -}}
        --set hostPort.enabled=true
        {{- end}}
      _IPAM_LB: >-
        {{if eq .IPAM_LB "yes" -}}
        --set nodeIPAM.enabled=true
        {{- end}}
      _EXTERNAL_LB: >-
        {{if eq .LB "yes" -}}
        --set externalIPs.enabled=true
        {{- end}}
      _SOCKET_LB: >-
        {{if eq .SOCKET_LB "yes" -}}
        --set socketLB.enabled=true
        --set socketLB.hostNamespaceOnly=true
        {{- end}}
      _FIREWALL: >-
        {{if eq .FIREWALL "yes" -}}
        --set hostFirewall.enabled=true
        {{- end}}
      _PROMETHEUS: >-
        {{if eq .PROMETHEUS "yes" -}}
        --set prometheus.enabled=true
        --set operator.prometheus.enabled=true
        {{- end}}
      _GRAFANA: >-
        {{if eq .GRAFANA "yes" -}}
        --set dashboards.enabled=true
        {{- end}}
      _OPERATOR: >-
        {{if .REPLICAS -}}
        --set operator.replicas={{.REPLICAS}}
        {{- else -}}
        --set operator.replicas=1
        {{- end}}
    cmds:
      - helm repo add cilium https://helm.cilium.io/ --force-update
      - >
        helm upgrade --install
        cilium cilium/cilium
        --version {{._VERSION}}
        --namespace kube-system
        --reuse-values
        {{._POD_CIDR}}
        {{._ISTIO}}
        {{._INGRESS}}
        {{._EGRESS}}
        {{._GATEWAY_API}}
        {{._PROMETHEUS}}
        {{._GRAFANA}}
        {{._OPERATOR}}
        {{.CLI_ARGS}}
      # Restart unmanaged Pods
      # - >-
      #   kubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,HOSTNETWORK:.spec.hostNetwork --no-headers=true
      #   | grep '<none>'
      #   | awk '{print "-n "$1" "$2}'
      #   | xargs -L 1 -r kubectl delete pod
      - >-
        {{if eq ._INGRESS "yes" -}}
        kubectl -n kube-system rollout restart deployment/cilium-operator;
        kubectl -n kube-system rollout restart ds/cilium;
        {{else if eq ._GATEWAY_API "yes" -}}
        kubectl -n kube-system rollout restart deployment/cilium-operator;
        kubectl -n kube-system rollout restart ds/cilium;
        {{else if eq ._IPAM "yes" -}}
        kubectl -n kube-system rollout restart deployment/cilium-operator;
        {{- end}}

  cilium:check:
    cmds:
      - cilium status --wait
      - cilium connectivity test

  cilium:k3s:stop:
    desc: >-
      Before stopping k3s cluster,
      you MUST remove cilium network interfaces and iptables rules.
      If you fail to do this,
      you may lose network connectivity to the host when K3s is stopped
    cmds:
      - ip link delete cilium_host
      - ip link delete cilium_net
      - ip link delete cilium_vxlan
      - iptables-save | grep -iv cilium | iptables-restore
      - ip6tables-save | grep -iv cilium | ip6tables-restore

  cnpg:deploy:self:
    cmds:
      - helm repo add cnpg https://cloudnative-pg.github.io/charts --force-update
      - >-
        helm upgrade --install
        cnpg cnpg/cloudnative-pg
        --namespace cnpg-system --create-namespace
        --reuse-values
        {{.CLI_ARGS}}

  cnpg:deploy:cluster:
    desc: >-
      Deploy postgres cluster
      [
      RELEASE
      NAMESPACE
      POSTGIS
      PG_VERSION
      INSTANCES
      SIZE
      STORAGE_CLASS
      S3
      AWS_ENDPOINTS
      AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_BUCKET
      AWS_REGION
      AWS_PATH
      CRON
      RETENTION
      ]
      -- [
      helm_upgrade_options
      ]
    vars:
      _NAME: '{{default "postgres" .NAME}}'
      _NAMESPACE: '{{default "database" .NAMESPACE}}'
      _POSTGIS: >-
        {{if eq .POSTGIS "yes" -}}
        --set type=postgis
        {{- end}}
      _PG_VERSION: '--set-string version.postgresql={{default 16 .PG_VERSION}}'
      # `cluster` mode
      _INSTANCES: '--set cluster.instances={{default 1 .INSTANCES}}'
      _SIZE: '--set cluster.storage.size={{default "8Gi" .SIZE}}'
      _STORAGE_CLASS: >-
        {{if .STORAGE_CLASS -}}
        --set cluster.storage.storageClass={{.STORAGE_CLASS}}
        {{- end}}
      # Backup
      _S3: >-
        {{if eq .S3 "true" -}}
        --set backups.enabled=true
        --set backups.provider=s3
        --set backups.endpointURL={{.AWS_ENDPOINTS}}
        --set backups.s3.region="{{default "us-east-1" .AWS_REGION}}"
        --set backups.s3.accessKey={{.AWS_ACCESS_KEY_ID}}
        --set backups.s3.secretKey={{.AWS_SECRET_ACCESS_KEY}}
        --set backups.s3.bucket={{.AWS_BUCKET}}
        --set backups.s3.path="{{default "/" .AWS_PATH}}"
        --set backups.scheduledBackups[0].name={{default "daily-backup" .BACKUP_NAME}}
        --set backups.scheduledBackups[0].backupOwnerReference=self
        --set backups.scheduledBackups[0].method=barmanObjectStore
        --set backups.scheduledBackups[0].schedule="{{default "0 0 3 * * *" .CRON}}"
        --set backups.retentionPolicy={{default "30d" .RETENTION}}
        {{- end}}
      _RESOURCES: >-
        --set cluster.resources.limits.cpu=700m
        --set cluster.resources.limits.memory=700Mi
        --set cluster.resources.requests.cpu=200m
        --set cluster.resources.requests.memory=200Mi
    cmds:
      - helm repo add cnpg https://cloudnative-pg.github.io/charts --force-update
      - >
        helm upgrade --install
        {{._NAME}} cnpg/cluster
        --namespace {{._NAMESPACE}} --create-namespace
        --reuse-values
        {{._POSTGIS}}
        {{._PG_VERSION}}
        {{._RESOURCES}}
        {{._INSTANCES}}
        {{._SIZE}}
        {{._STORAGE_CLASS}}
        {{._S3}}
        {{.CLI_ARGS}}

  consul:deploy:self:
    cmds:
      - helm repo add hashicorp https://helm.releases.hashicorp.com --force-update
      - >-
        helm upgrade --install
        consul hashicorp/consul
        --version 1.5.3
        --create-namespace --namespace consul
        --reuse-values
        --set global.name=consul
        --set connectInject.enabled=true
        --set ingressGateways.enabled=ture
        {{.CLI_ARGS}}

  csi:nfs:deploy:self:
    _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}v4.9.0{{end}}'
    cmds:
      - helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts
      - helm repo update
      - >-
        helm upgrade --install
        csi-driver-nfs csi-driver-nfs/csi-driver-nfs
        --version {{._VERSION}}
        --namespace kube-system
        --set storageClass.create=true
        {{.CLI_ARGS}}

  csi:s3:deploy:self:
    requires:
      vars:
        - AWS_ENDPOINTS
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - AWS_BUCKET
    vars:
      _S3_REGION: >-
        {{if .S3_REGION -}}
        --set secret.region={{.S3_REGION}}
        {{- end}}
      _RELEASE: '{{default "csi-s3" .RELEASE}}'
    cmds:
      - helm repo add yandex-s3 https://yandex-cloud.github.io/k8s-csi-s3/charts --force-update
      - >
        helm upgrade --install
        {{._RELEASE}} yandex-s3/csi-s3
        -n csi-s3 --create-namespace
        --reuse-values
        {{._S3_REGION}}
        --set secret.endpoint={{.AWS_ENDPOINTS}}
        --set secret.accessKey={{.AWS_ACCESS_KEY_ID}}
        --set secret.secretKey={{.AWS_SECRET_ACCESS_KEY}}
        --set storageClass.singleBucket={{.AWS_BUCKET}}
        --set storageClass.reclaimPolicy={{default "Retain" .RECLAIM_POLICY}}
        --set storageClass.mountOptions='--memory-limit 1000 --dir-mode 0777 --file-mode 0666 --no-systemd --enable-mtime --enable-perms'

  csi:smb:deploy:self:
    _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}v1.16.0{{end}}'
    cmds:
      - helm repo add csi-driver-smb https://raw.githubusercontent.com/kubernetes-csi/csi-driver-smb/master/charts
      - helm repo update
      - >-
        helm upgrade --install
        csi-driver-smb csi-driver-smb/csi-driver-smb
        --version {{._VERSION}}
        --namespace kube-system
        {{.CLI_ARGS}}

  csi:seasweedfs:deploy:self:
    _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}0.2.2{{end}}'
    cmds:
      - helm repo add --force-update seaweedfs-csi-driver https://seaweedfs.github.io/seaweedfs-csi-driver/helm
      - >-
        helm upgrade --install
        seaweedfs-csi-driver seaweedfs-csi-driver/seaweedfs-csi-driver
        --version {{._VERSION}}
        {{.CLI_ARGS}}

  devtron:deploy:self:
    vars:
      _VERSION: '{{default "0.22.78" .VERSION}}'
    cmds:
      - helm repo add devtron https://helm.devtron.ai --force-update
      - >-
        helm upgrade --install
        devtron devtron/devtron-operator
        --version {{._VERSION}}
        -n devtroncd --create-namespace

  directus:deploy:self:
    cmds:
      - helm repo add directus https://directus-labs.github.io/helm-chart/ --force-update
      - >-
        helm upgrade --install
        directus directus/directus
        -n directus --create-namespace
        --reuse-values
        {{.CLI_ARGS}}

  envoy-gateway:deploy:self:
    desc: 'Deploy Envoy Gateway that supports gateway api v1.1.0'
    cmds:
      - >-
        helm upgrade --install
        eg oci://docker.io/envoyproxy/gateway-helm
        --version v1.1.2
        --namespace envoy-gateway-system --create-namespace
        --reuse-values
        {{.CLI_ARGS}}
      - >-
        kubectl wait
        --timeout=5m
        --namespace envoy-gateway-system
        deployment/envoy-gateway
        --for=condition=Available

  envoy-gateway:deploy:gateway:web:
    desc: >-
      [
      GATEWAY_SERVICE_TYPE
      NAMESPACE
      CERT_NAME
      CERT_NAMESPACE
      ]
    vars:
      _NAMESPACE: >-
        {{default "envoy-gateway-system" .NAMESPACE}}
    cmds:
      - |-
        kubectl apply -f - <<EOF
        {{if .GATEWAY_SERVICE_TYPE}}
        ---
        apiVersion: gateway.envoyproxy.io/v1alpha1
        kind: EnvoyProxy
        metadata:
          name: custom-eg-config
          namespace: {{._NAMESPACE}}
        spec:
          provider:
            type: Kubernetes
            kubernetes:
              envoyService:
                type: {{.GATEWAY_SERVICE_TYPE}}
        {{end}}
        ---
        apiVersion: gateway.networking.k8s.io/v1
        kind: GatewayClass
        metadata:
          name: envoy-gateway
          namespace: {{._NAMESPACE}}
        spec:
          controllerName: gateway.envoyproxy.io/gatewayclass-controller
          {{- if .GATEWAY_SERVICE_TYPE}}
          parametersRef:
            group: gateway.envoyproxy.io
            kind: EnvoyProxy
            name: custom-eg-config
            namespace: {{._NAMESPACE}}
          {{- end}}
        ---
        apiVersion: gateway.networking.k8s.io/v1
        kind: Gateway
        metadata:
          name: envoy-gateway-web
          namespace: {{._NAMESPACE}}
        spec:
          gatewayClassName: envoy-gateway
          listeners:
            - name: http
              protocol: HTTP
              port: 80
              allowedRoutes:
                namespaces:
                  from: Selector
                  selector:
                    matchLabels:
                      gateway-namespace: "true"
            - name: https
              protocol: HTTPS
              port: 443
              tls:
                mode: Terminate
                certificateRefs:
                  - name: {{.CERT_NAME}}
                    namespace: {{default ._NAMESPACE .CERT_NAMESPACE}}
                    kind: Secret
              allowedRoutes:
                namespaces:
                  from: Selector
                  selector:
                    matchLabels:
                      gateway-namespace: "true"
        EOF

  fission:deploy:self:
    vars:
      _VERSION: 'v1.20.4'
      _NAMESPACE: fission
    cmds:
      - kubectl create namespace {{._NAMESPACE}}
      - kubectl create -k "github.com/fission/fission/crds/v1?ref={{._VERSION}}"
      - helm repo add fission-charts https://fission.github.io/fission-charts/
      - helm repo update
      - >
        helm upgrade --install
        fission fission-charts/fission-all
        --version {{._VERSION}}
        --namespace {{._NAMESPACE}} --create-namespace

  flux:deploy:self:
    desc: 'Deploy Flux CD controllers to cluster [CONTEXT]'
    preconditions:
      - flux check --pre
    vars:
      _CONTEXT: '{{if .CONTEXT}}--context {{.CONTEXT}}{{end}}'
    cmds:
      - flux install {{._CONTEXT}}

  flux:deploy:community:
    desc: 'Deploy Flux CD controllers to cluster [NAME CONTEXT]'
    preconditions:
      - helm version
      - flux check --pre
    vars:
      _NAME: '{{if .NAME}}{{.NAME}}{{else}}flux{{end}}'
      _CONTEXT: '{{if .CONTEXT}}--kube-context {{.CONTEXT}}{{end}}'
    cmds:
      - helm repo add fluxcd-community https://fluxcd-community.github.io/helm-charts
      - >
        helm upgrade --install
        {{._CONTEXT}}
        --namespace flux-system --create-namespace
        {{._NAME}} fluxcd-community/flux2
      - flux check

  gateway-api:install:v1.1.0:
    cmds:
      - kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml

  helm:repo:add:
    desc: 'Add a repo <REPO>'
    internal: true
    requires: {vars: [REPO]}
    cmds:
      - cmd: 'helm repo add {{.REPO}}'

  helm:repo:add:bulk:
    desc: 'Add repos <REPOS>'
    internal: true
    requires: {var: [REPOS]}
    cmds:
      - task: 'helm:repo:add'
        vars: {REPO: '{{.REPO}}'}
        for: {var: REPOS, as: REPO}
      - cmd: helm repo update

  helm:repo:add:curated-list:
    desc: 'Import repos'
    cmds:
      - task: 'helm:repo:add:bulk'
        vars:
          REPOS:
            - authentik https://charts.goauthentik.io
            - bitnami https://charts.bitnami.com/bitnami
            - datawire https://app.getambassador.io
            - fluxcd https://charts.fluxcd.io
            - fluxcd-community https://fluxcd-community.github.io/helm-charts
            - prometheus-community https://prometheus-community.github.io/helm-charts

  hwameistor:deploy:self:
    cmds:
      - helm repo add hwameistor-operator https://hwameistor.io/hwameistor-operator
      - helm repo update
      - >
        helm upgrade --install
        hwameistor-operator hwameistor-operator/hwameistor-operator
        --namespace hwameistor --create-namespace
        {{.CLI_ARGS}}

  istio:deploy:self:
    cmds:
      - >-
        helm upgrade --install
        istio-base istio/base
        -n istio-system --create-namespace
        --wait
      - >-
        helm upgrade --install
        istiod istio/istiod
        -n istio-system --create-namespace
        --set profile=ambient
        --wait
      - >-
        helm upgrade --install
        istio-ingress istio/gateway
        -n istio-system --create-namespace
        --wait

  istio:deploy:self:default:
    cmds:
      # Install the control plane
      - helm repo add istio https://istio-release.storage.googleapis.com/charts
      - helm repo update
      - helm upgrade --install -n istio-system --create-namespace istio-base istio/base --wait
      - kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml
      - helm upgrade --install -n istio-system istiod istio/istiod --wait
      # Install the data plane
      - helm upgrade --install -n istio-ingress --create-namespace istio-ingress istio/gateway --wait

  istio:deploy:self:minimal:
    cmds:
      # Install the control plane
      - helm repo add istio https://istio-release.storage.googleapis.com/charts
      - helm repo update
      - helm upgrade --install -n istio-system istiod istio/istiod --wait
      - kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml

  istio:deploy:self:ambient:
    cmds:
      # Install the control plane
      - helm repo add istio https://istio-release.storage.googleapis.com/charts
      - helm repo update
      - helm upgrade --install -n istio-system --create-namespace istio-base istio/base --wait
      - kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml
      - helm upgrade --install -n istio-system istiod istio/istiod --set profile=ambient --wait
      - helm upgrade --install -n kube-system istio-cni istio/cni -n kube-system
      # Install the data plane
      - helm upgrade --install ztunnel istio/ztunnel --wait
      # - helm upgrade --install -n istio-ingress --create-namespace istio-ingress istio/gateway --wait
      # - helm upgrade --install -n istio-system istio-egress istio/egressgateway

  istio:deploy:self:istioctl:
    cmds:
      # - istioctl install --set profile=ambient --skip-confirmation
      - istioctl install --set profile=default --skip-confirmation
      - istioctl verify-install

  juicefs:csi:deploy:self:
    cmds:
      - helm repo add juicefs https://juicedata.github.io/charts/ --force-update
      - >
        helm upgrade --install
        juicefs-csi-driver juicefs/juicefs-csi-driver
        --namespace kube-system --create-namespace
        --reuse-values
        --set metrics.enabled=true
        --set controller.replicas=1
        {{.CLI_ARGS}}

  juicefs:csi:deploy:storage-class:
    cmds:
      - |-
        helm upgrade --install juicefs juicefs/juicefs-csi-driver --namespace kube-system --create-namespace --reuse-values --values - <<'EOF'
        # For production environment, manually create & manage storageClass outside Helm is recommended, ref: https://juicefs.com/docs/csi/guide/pv#create-storage-class
        storageClasses:
        - name: "juicefs-sc"
          # Set to true to actually create this StorageClass
          enabled: true
          # Either Retain or Delete, ref: https://juicefs.com/docs/csi/guide/resource-optimization#reclaim-policy
          reclaimPolicy: Retain
          # Set to true to allow PVC expansion
          allowVolumeExpansion: true
          # Additional annotations for this StorageClass, e.g. make it default
          # annotations:
          #   storageclass.kubernetes.io/is-default-class: "true"

          backend:
            # The JuiceFS file system name
            name: ""
            # Connection URL for metadata engine (e.g. Redis), for community edition use only, ref: https://juicefs.com/docs/community/databases_for_metadata
            metaurl: ""
            # Object storage type, such as s3, gs, oss, for community edition use only, ref: https://juicefs.com/docs/community/how_to_setup_object_storage
            storage: ""
            # Bucket URL, for community edition use only, ref: https://juicefs.com/docs/community/how_to_setup_object_storage
            bucket: ""
            # Token for JuiceFS Enterprise Edition token, ref: https://juicefs.com/docs/cloud/acl
            token: ""
            # Access key for object storage
            accessKey: ""
            # Secret key for object storage
            secretKey: ""
            # Environment variables for the JuiceFS Client
            # Example: {"a": "b"}
            # Ref: https://juicefs.com/docs/csi/guide/pv#volume-credentials
            envs: ""
            # Extra files for the mount pod, ref: https://juicefs.com/docs/csi/guide/pv/#mount-pod-extra-files
            configs: ""
            # The number of days which files are kept in the trash, for community edition use only, ref: https://juicefs.com/docs/community/security/trash
            trashDays: ""
            # Options passed to the "juicefs format" or "juicefs auth" command, depending on which edition you're using
            # Example: block-size=4096,capacity=10
            # Ref: https://juicefs.com/docs/community/command_reference#format and https://juicefs.com/docs/cloud/reference/commands_reference#auth
            formatOptions: ""

          # Options for the "juicefs mount" command
          # Example:
          # - debug
          # - cache-size=2048
          # - cache-dir=/var/foo
          # Ref: https://juicefs.com/docs/community/command_reference#mount and https://juicefs.com/docs/cloud/reference/commands_reference#mount
          mountOptions:
          # Customize PV directory format, ref: https://juicefs.com/docs/csi/guide/pv#using-path-pattern
          # If enabled, controller.provisioner must be set to true
          # Example: "${.PVC.namespace}-${.PVC.name}"
          pathPattern: ""

          # Using PVC as JuiceFS cache path, ref: https://juicefs.com/docs/csi/guide/cache#use-pvc-as-cache-path
          cachePVC: ""

          mountPod:
            # Mount pod resource requests & limits
            resources:
              limits:
                cpu: 5000m
                memory: 5Gi
              requests:
                cpu: 100m
                memory: 256Mi
            # Override mount pod image, ref: https://juicefs.com/docs/csi/guide/custom-image
            image: ""
            # Set annotations for the mount pod
            annotations: {}
        EOF

  juicefs:csi:format:
    desc: >-
      Create secret of JuiceFS
      <AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      BUCKET_URL
      DATABASE>
      [SECRET
      FS_NAME
      STORAGE
      ENCRYPT_KEY
      JFS_RSA_PASSPHRASE]
    requires:
      vars:
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - BUCKET_URL # https://<BUCKET>.s3.<REGION>.amazonaws.com
        - DATABASE
    vars:
      _SECRET: >-
        {{if .SECRET -}}
        {{.SECRET}}
        {{- else -}}
        juicefs-s3-secret
        {{- end}}
      _FS_NAME: >-
        {{if .FS_NAME -}}
        {{.FS_NAME}}
        {{- else -}}
        juicefs
        {{- end}}
      # file/s3/gs/oss/cos
      _STORAGE: >-
        {{if .STORAGE -}}
        {{.STORAGE}}
        {{- else -}}
        s3
        {{- end}}
      _TZ: >-
        {{if .TZ -}}
        {{.TZ}}
        {{- else -}}
        UTC
        {{- end}}
      # path/to/key.pem
      _ENCRYPT_RSA_KEY: >-
        {{if .ENCRYPT_RSA_KEY -}}
        ,--encrypt-rsa-key={{.ENCRYPT_KEY}}
        {{- end}}
    cmds:
      - |-
        kubectl apply -n default -f - <<'EOF'
        apiVersion: v1
        kind: Secret
        metadata:
          name: {{._SECRET}}
          namespace: default
          labels:
            juicefs.com/validate-secret: "true"
        type: Opaque
        stringData:
          name: {{._FS_NAME}}
          metaurl: {{.DATABASE}}
          storage: {{._STORAGE}}
          bucket: {{.BUCKET_URL}}
          access-key: {{.AWS_ACCESS_KEY_ID}}
          secret-key: {{.AWS_SECRET_ACCESS_KEY}}
          envs: "{TZ: {{._TZ}}{{if .JFS_RSA_PASSPHRASE}}, JFS_RSA_PASSPHRASE: {{.JFS_RSA_PASSPHRASE}}{{end}}}"
          format-options: --block-size=2M,--compress=lz4,trash-days=3{{._ENCRYPT_RSA_KEY}}
        EOF

  kubectl:exec:shell:
    requires:
      vars:
        - NAMESPACE
        - POD
        - CONTAINER
    cmds:
      - >-
        kubectl exec
        -i -t
        -n {{.NAMESPACE}}
        {{.POD}}
        --container {{.CONTAINER}}
        --
        sh -c
        "clear; (bash || ash || sh)"

  kadalu:deploy:self:
    vars:
      _VERSION: '1.3.0'
    cmds:
      - >
        helm upgrade --install
        kadalu https://github.com/kadalu/kadalu/releases/download/1.3.0/kadalu-helm-chart.tgz
        --namespace kadalu --create-namespace
        --set kubernetesDistro=kubernetes
        --set operator.enabled=true
        {{.CLI_ARGS}}

  kanister:deploy:self:
    cmds:
      - helm repo add kanister https://charts.kanister.io/
      - helm repo update
      - >
        helm upgrade --install
        kanister kanister/kanister-operator
        --namespace kanister --create-namespace

  k3sup-init-master:
    desc: >-
      Initial k3s cluster via SSH
      [
      IS_LOCAL
      IS_SSH
      SSH_HOST
      SSH_PORT
      SSH_USER
      SSH_KEY

      IP
      DOMAIN
      NODE_NAME

      VERSION
      IS_EMBEDDED_ETCD
      IS_MERGE_EXISTING

      DATA_DIR
      IS_DISABLE
      IS_TRAEFIK
      IS_SERVICELB
      DISABLE
      IS_CUSTOM_CNI
      IS_ENCRYPT
      CRON
      IS_S3
      AWS_ENDPOINTS
      AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_BUCKET
      AWS_REGION
      AWS_FOLDER
      IS_INSECURE

      CONTEXT
      ]
      -- [options]
    cmds:
      - cmd: >-
          k3sup install
          {{._LOCAL}}
          {{._SSH}}
          {{._NODE_INFO}}
          {{._VERSION}}
          {{._DB}}
          {{._MERGE}}
          {{._K3S}}
          {{._PRINT}}
          {{._CONTEXT}}
          {{.CLI_ARGS}}
    vars:
      _LOCAL: >-
        {{if eq .IS_LOCAL "yes" -}}
        --local
        {{- end}}
      _SSH: >-
        {{if eq .IS_SSH "yes" -}}
        --host {{default "127.0.0.1" .SSH_HOST}}
        --ssh-port {{default "22" .SSH_PORT}}
        --user {{default .USER .SSH_USER}}
        {{if .SSH_KEY}}--ssh-key {{.SSH_KEY}}{{end}}
        {{- end}}
      _NODE_INFO: >-
        {{if .IP -}}
        --ip {{.IP}}
        {{- end}}
        {{if .DOMAIN -}}
        --host {{.DOMAIN}}
        {{- end}}
      _VERSION: >-
        {{if .VERSION -}}
        --k3s-version {{.VERSION}}
        {{- else -}}
        --k3s-channel stable
        {{- end}}
      _DB: >-
        {{if ne .IS_EMBEDDED_ETCD "no" -}}
        --cluster
        {{- end}}
      _MERGE: >-
        {{if ne .IS_MERGE_EXISTING "no" -}}
        --merge
        {{- end}}
      _K3S: >-
        --k3s-extra-args '--data-dir {{default "/var/lib/rancher/k3s" .DATA_DIR}}
        {{if eq .IS_DISABLE "yes" -}}
        --disable
        {{if eq .IS_TRAEFIK "no" -}}
        traefik
        {{- end -}}
        {{if eq .IS_SERVICELB "no" -}}
        ,servicelb
        {{- end}}
        {{if .DISABLE -}}
        ,{{.DISABLE}}
        {{- end}}
        {{- end}}
        {{if eq .IS_CUSTOM_CNI "yes" -}}
        --flannel-backend=none
        --disable-network-policy
        {{- end}}
        {{if eq .IS_ENCRYPT "yes" -}}
        --secrets-encryption
        {{- end}}
        {{if .NODE_NAME -}}
        --node-name {{.NODE_NAME}}
        {{- end}}
        {{if .LOG -}}
        --log {{.LOG}}
        {{- end}}
        --etcd-snapshot-schedule-cron "{{default "0 */12 * * *" .CRON}}"
        {{if eq .IS_S3 "yes" -}}
        --etcd-s3
        --etcd-s3-endpoint {{default "s3.amazonaws.com" .AWS_ENDPOINTS}}
        --etcd-s3-access-key {{.AWS_ACCESS_KEY_ID}}
        --etcd-s3-secret-key {{.AWS_SECRET_ACCESS_KEY}}
        --etcd-s3-bucket {{.AWS_BUCKET}}
        --etcd-s3-region {{default "us-east-1" .AWS_REGION}}
        {{if .AWS_FOLDER}}--etcd-s3-folder {{.AWS_FOLDER}}{{end}}
        {{if eq .IS_INSECURE "yes"}}--etcd-s3-insecure{{end}}
        {{- end}}
        '
      _PRINT: --print-command
      _CONTEXT: --context {{default "default" .CONTEXT}}

  # TODO: improvement
  k3sup:join:master:
    desc: >-
      <
      CLUSTER_IP
      TOKEN

      SSH_IP
      SSH_PORT
      SSH_USER
      SSH_KEY
      NODE_NAME

      AWS_ENDPOINTS
      AWS_REGION
      AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_BUCKET
      >
      -- [OPTIONS]
    vars:
      _CLUSTER: >-
        --server-url https://{{.CLUSTER_IP}}:6443
        --node-token '{{.TOKEN}}'
      _NODE: >-
        --ip '{{.SSH_IP}}'
        --user {{.SSH_USER}}
        --ssh-port {{.SSH_PORT}}
        --ssh-key '{{.SSH_KEY}}'
      _NODE_TYPE: '--server'
      _K3S_ARGS: >-
        --server-data-dir '/home/rancher/k3s'
        --k3s-version v1.28.14+k3s1
        --k3s-extra-args
        '
        --disable traefik,servicelb,local-storage
        --flannel-backend none
        --disable-network-policy
        --secrets-encryption
        --node-name {{.NODE_NAME}}
        --etcd-snapshot-schedule-cron "44 */8 * * *"
        --etcd-snapshot-retention 10
        --etcd-s3
        --etcd-s3-endpoint {{.AWS_ENDPOINTS}}
        --etcd-s3-region {{default "us-east-1" .AWS_REGION}}
        --etcd-s3-access-key {{.AWS_ACCESS_KEY_ID}}
        --etcd-s3-secret-key {{.AWS_SECRET_ACCESS_KEY}}
        --etcd-s3-bucket {{.AWS_BUCKET}}
        '
    cmds:
      - cmd: >-
          k3sup join
          --print-command
          {{._CLUSTER}}
          {{._NODE}}
          {{._NODE_TYPE}}
          {{._K3S_ARGS}}
          {{.CLI_ARGS}}

  k3sup-join-worker:
    desc: 'Join a k3s cluster via SSH <SSH_HOST SSH_USER MASTER_SSH_HOST MASTER_SSH_PORT>'
    cmds:
      - cmd: >
          k3sup join
          --server
          --host {{.SSH_HOST}}
          --user {{.SSH_USER}}
          --server-host {{.MASTER_SSH_HOST}}
          --server-user {{.MASTER_SSH_PORT}}
          --k3s-channel stable

  karpor:deploy:self:
    desc: 'Deploy karpor [STORAGE_CLASS]'
    vars:
      _STORAGE_CLASS: >-
        {{if .STORAGE_CLASS -}}
        --set etcd.persistence.storageClass={{.STORAGE_CLASS}}
        {{- end}}
    cmds:
      - helm repo add kusionstack https://kusionstack.github.io/charts --force-update
      - >-
        helm upgrade --install
        karpor kusionstack/karpor
        --reuse-values
        {{._STORAGE_CLASS}}
        --set server.resources.requests.cpu=100m
        --set syncer.resources.requests.cpu=100m
        --set elasticsearch.resources.requests.cpu=100m
        --set etcd.resources.requests.cpu=100m
        {{.CLI_ARGS}}

  karpor:get:config:
    cmds:
      - >-
        kubectl get configmap karpor-kubeconfig -n karpor -o go-template='{{.data.config}}'
        > $HOME/.kube/karpor-hub-cluster.kubeconfig

  karpo:create:sa:admin:
    env:
      KUBECONFIG: '$HOME/.kube/karpor-hub-cluster.kubeconfig'
    cmds:
      - kubectl create serviceaccount karpor-admin
      - kubectl create clusterrolebinding karpor-admin --clusterrole=karpor-admin --serviceaccount=default:karpor-admin

  karpo:create:sa:guest:
    env:
      KUBECONFIG: '$HOME/.kube/karpor-hub-cluster.kubeconfig'
    cmds:
      - kubectl create serviceaccount karpor-guest
      - kubectl create clusterrolebinding karpor-guest --clusterrole=karpor-guest --serviceaccount=default:karpor-guest

  karpo:create:token:admin:
    env:
      KUBECONFIG: '$HOME/.kube/karpor-hub-cluster.kubeconfig'
    cmds:
      - kubectl create token karpor-admin --duration=1000h

  karpo:create:token:guest:
    env:
      KUBECONFIG: '$HOME/.kube/karpor-hub-cluster.kubeconfig'
    cmds:
      - kubectl create token karpor-admin --duration=8760h

  kong:deploy:self:
    cmds:
      - >-
        helm upgrade --install
        kong --repo https://charts.konghq.com ingress
        --namespace kong --create-namespace
        --reuse-values
        {{.CLI_ARGS}}

  krew:install:plugin:
    desc: 'Install a kubectl plugin [PLUGIN] -- [plugin]'
    vars:
      _PLUGIN: '{{if .PLUGIN}}{{.PLUGIN}}{{else if .CLI_ARGS}}{{.CLI_ARGS}}{{else}}--help{{end}}'
    cmds:
      - kubectl krew install {{._PLUGIN}}

  krew:install:plugins:
    internal: true
    requires:
      vars: [PLUGINS]
    cmds:
      - task: krew:install:plugin
        vars:
          PLUGIN: '{{.PLUGIN}}'
        for: {var: PLUGINS, as: PLUGIN}

  krew:install:plugin:curated-list:
    desc: 'Install recommended plugins'
    cmds:
      - task: krew:install:plugins
        vars:
          PLUGINS:
            - oidc-login
            - access-matrix
            - cert-manager
            - cilium
            - cnpg
            - cost
            - explore
            - flame
            - kc
            - pv-migrate
            - stern
            - tree
            - vela

  krew:install:plugin:curated-list:unix:
    platforms: [linux, darwin]
    desc: 'Install recommended plugins'
    cmds:
      - task: krew:install:plugins
        vars:
          PLUGINS:
            - ctx
            - ns
            - node-shell

  kuboard:deploy:self:
    cmds:
      - kubectl apply -f https://addons.kuboard.cn/kuboard/kuboard-v3.yaml

  kube-vip:deploy:self:
    desc: 'Deploy kube-vip'
    cmds:
      - kubectl apply -f https://kube-vip.io/manifests/rbac.yaml
      - 'echo "TODO: DaemonSet"'
      - echo "To simplify, is is recommended to use loadBalancer to replace kube-vip"
      - echo "k3d comes with HA implemented by Nginx"

  kubero:deploy:self:full:
    cmds:
      - task: kubero:deploy:operator:full
      - task: kubero:deploy:ui

  kubero:deploy:self:minimal:
    cmds:
      - task: kubero:deploy:operator:minimal
      - task: kubero:deploy:ui

  kubero:deploy:operator:minimal:
    cmds:
      - kubectl apply -f https://raw.githubusercontent.com/kubero-dev/kubero-operator/main/deploy/operator.yaml

  kubero:deploy:operator:full:
    cmds:
      - kubectl create -f https://raw.githubusercontent.com/operator-framework/operator-lifecycle-manager/master/deploy/upstream/quickstart/crds.yaml
      - kubectl create -f https://raw.githubusercontent.com/operator-framework/operator-lifecycle-manager/master/deploy/upstream/quickstart/olm.yaml
      - kubectl create -f https://operatorhub.io/install/kubero-operator.yaml

  kubero:deploy:ui:
    requires:
      vars:
        - 'KUBERO_WEBHOOK_SECRET' # openssl rand -hex 20
        - 'KUBERO_SESSION_KEY' # openssl rand -hex 20
    cmds:
      - kubectl create namespace kubero
      - >
        kubectl create secret generic kubero-secrets
        -n kubero
        --from-literal=KUBERO_WEBHOOK_SECRET={{.KUBERO_WEBHOOK_SECRET}}
        --from-literal=KUBERO_SESSION_KEY={{.KUBERO_SESSION_KEY}}
        {{.CLI_ARGS}}
      - >
        kubectl apply
        -f https://raw.githubusercontent.com/kubero-dev/kubero-operator/main/config/samples/application_v1alpha1_kubero.yaml
        -n kubero

  kubeshark:deploy:self:
    cmds:
      - helm repo add kubeshark https://helm.kubeshark.co
      - helm repo update
      - >
        helm upgrade --install
        kubeshark kubeshark/kubeshark
        --namespace kubeshark --create-namespace

  kubesphere:deploy:self:
    desc: 'Deploy KubeSphere. http://ip:30880 with "admin" "P@88w0rd"'
    cmds:
      - >
        helm upgrade --install
        ks-core https://charts.kubesphere.io/main/ks-core-1.1.2.tgz
        -n kubesphere-system --create-namespace
        --debug
        --wait
        {{.CLI_ARGS}}

  kubevela:deploy:self:
    cmds:
      - helm repo add kubevela https://kubevela.github.io/charts --force-update
      - >
        helm upgrade --install
        kubevela kubevela/vela-core
        -n vela-system --create-namespace
        --wait

  kubevela:addon:registry:official:
    vars:
      VELA:
        sh: >-
          if command -v vela > /dev/null 2>&1; then
            echo -n "vela";
          elif command -v kubectl > /dev/null 2>&1; then
            echo -n "kubectl vela";
          fi;
    cmds:
      - >-
        {{.VELA}} addon registry update
        KubeVela --type helm --endpoint=https://kubevela.github.io/catalog/official
      - >-
        {{.VELA}} addon registry update
        KubeVela --type helm --endpoint=https://kubevela.github.io/catalog/experimental

  kubevela:addon:velaux:install:
    vars:
      VELA:
        sh: >-
          if command -v vela > /dev/null 2>&1; then
            echo -n "vela";
          elif command -v kubectl > /dev/null 2>&1; then
            echo -n "kubectl vela";
          fi;
      _GATEWAY: >-
        {{if eq .IS_TRAEFIK "true" -}}
        gatewayDriver="traefik"
        {{- end}}
        {{if .DOMAIN -}}
        domain="{{.DOMAIN}}"
        {{- end}}
      _DB: >-
        {{if eq .IS_POSTGRES "true" -}}
        dbType="postgres"
        {{- end}}
        {{if .DB_NAME -}}
        database="{{.DB_NAME}}"
        {{- end}}
        {{if .DB_URL -}}
        dbURL="{{.DB_URL}}"
        {{- end}}
    cmds:
      - >-
        {{.VELA}} addon enable velaux
        {{._DB}}
        {{._GATEWAY}}

  kubevela:addon:velaux:forward:
    vars:
      VELA:
        sh: >-
          if command -v vela > /dev/null 2>&1; then
            echo -n "vela";
          elif command -v kubectl > /dev/null 2>&1; then
            echo -n "kubectl vela";
          fi;
    cmds:
      - '{{.VELA}} port-forward -n vela-system addon-velaux 8000:8000'

  kubevela:crd:list:
    vars:
      VELA:
        sh: >-
          if command -v vela > /dev/null 2>&1; then
            echo -n "vela";
          elif command -v kubectl > /dev/null 2>&1; then
            echo -n "kubectl vela";
          fi;
    cmds:
      - >-
        {{.VELA}} def list

  kubevela:crd:describe:
    requires:
      vars: [KIND]
    vars:
      VELA:
        sh: >-
          if command -v vela > /dev/null 2>&1; then
            echo -n "vela";
          elif command -v kubectl > /dev/null 2>&1; then
            echo -n "kubectl vela";
          fi;
    cmds:
      - >-
        {{.VELA}} show {{.KIND}} {{.CLI_ARGS}}

  local-path-provisioner:deploy:self:k3s:
    vars:
      _DATA_DIR: '{{default "/var/lib/rancher/k3s/storage" .DATA_DIR}}'
    cmds:
      - task: local-path-provisioner:deploy:self
        vars:
          DATA_DIR: "{{._DATA_DIR}}"

  local-path-provisioner:deploy:self:
    dir: '{{.USER_WORKING_DIR}}'
    vars:
      _CSI_VERSION: '--set image.tag={{default "v0.0.30" .CSI_VERSION}}'
      _PATH: >-
        --set nodePathMap[0].node=DEFAULT_PATH_FOR_NON_LISTED_NODES
        --set nodePathMap[0].paths[0]='{{default "/opt/local-path-provisioner" .DATA_DIR}}'
      _SC: >-
        --set storageClass.defaultClass={{default "false" .DEFAULT}}
        --set storageClass.defaultVolumeType={{default "local" .VOLUME_TYPE}}
    cmds:
      - cmd: git clone https://github.com/rancher/local-path-provisioner.git
      - cmd: >-
          helm upgrade --install
          local-path-provisioner ./local-path-provisioner/deploy/chart/local-path-provisioner
          -n local-path-storage --create-namespace
          --reuse-values
          {{._CSI_VERSION}}
          {{._PATH}}
          {{._SC}}
          {{.CLI_ARGS}}
        ignore_error: true
      - defer: rm -r local-path-provisioner

  kmesh:deploy:self:
    desc: 'Deploy kmesh. Please install istio ahead'
    dir: '{{.USER_WORKING_DIR}}'
    cmds:
      - git clone https://github.com/kmesh-net/kmesh
      - >-
        helm upgrade install
        kmesh ./kmesh/deploy/charts/kmesh-helm
        -n kmesh-system --create-namespace
        --reuse-values
        {{.CLI_ARGS}}
      - rm -r kmesh

  kuma:deploy:self:
    desc: >-
      Deploy kuma
      [
      INGRESS
      INGRESS_SERVICE_TYPE
      EGRESS
      EGRESS_SERVICE_TYPE
      ]
      --
      [OPTIONS]
    vars:
      _RESOURCES: >-
        {{if ne .RESOURCES "no" -}}
        --set controlPlane.resources.requests.cpu=200m
        {{- end}}
      _INGRESS: >-
        {{if ne .INGRESS "no" -}}
        --set ingress.enabled=true
        --set ingress.service.type={{default "LoadBalancer" .INGRESS_SERVICE_TYPE}}
        {{- end}}
      _EGRESS: >-
        {{if eq .EGRESS "yes" -}}
        --set egress.enabled=true
        --set egress.service.type={{default "ClientIP" .EGRESS_SERVICE_TYPE}}
        {{- end}}
    cmds:
      - helm repo add --force-update kuma https://kumahq.github.io/charts
      - >-
        helm upgrade --install
        --create-namespace --namespace kuma-system
        kuma kuma/kuma
        --reuse-values
        {{._RESOURCES}}
        {{._INGRESS}}
        {{._EGRESS}}
        {{.CLI_ARGS}}

  manifest:validate:
    desc: 'Validate manifests -- <MANIFEST>...'
    dir: '{{.USER_WORKING_DIR}}'
    cmds:
      - >
        kubeconform
        -schema-location default
        -schema-location 'https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{"{{"}}.Group{{"}}"}}/{{"{{"}}.ResourceKind{{"}}"}}_{{"{{"}}.ResourceAPIVersion{{"}}"}}.json'
        {{.CLI_ARGS}}

  multus:deploy:self:k3s:whereabouts:
    desc: 'Deploy multus CNI [DATA_DIR]'
    vars:
      _DATA_DIR: '{{if .DATA_DIR}}{{.DATA_DIR}}{{else}}/var/lib/rancher/k3s{{end}}'
    cmds:
      - helm repo add rke2-charts https://rke2-charts.rancher.io
      - helm repo update
      - |-
        helm upgrade --install multus rke2-charts/rke2-multus -n kube-system --values - <<'EOF'
        config:
          cni_conf:
            confDir: {{._DATA_DIR}}/agent/etc/cni/net.d
            binDir: {{._DATA_DIR}}/data/current/bin/
            kubeconfig: {{._DATA_DIR}}/agent/etc/cni/net.d/multus.d/multus.kubeconfig
        rke2-whereabouts:
          fullnameOverride: whereabouts
          enabled: true
          cniConf:
            confDir: {{._DATA_DIR}}/agent/etc/cni/net.d
            binDir: {{._DATA_DIR}}/data/current/bin/
        EOF

  longhorn:prepare:deps:iscsi:
    platforms: [linux]
    cmds:
      - kubectl apply --wait -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.1/deploy/prerequisite/longhorn-iscsi-installation.yaml

  longhorn:prepare:deps:nfs:
    platforms: [linux]
    cmds:
      - kubectl apply --wait -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.1/deploy/prerequisite/longhorn-nfs-installation.yaml

  longhorn:prepare:deps:cryptsetup:
    platforms: [linux]
    desc: 'Optional'
    vars:
      OS:
        sh: >-
          OS=$(grep -E "^ID_LIKE=" /etc/os-release | cut -d '=' -f 2);
          if [[ -z "${OS}" ]];
          then OS=$(grep -E "^ID=" /etc/os-release | cut -d '=' -f 2);
          fi;
          echo -n ${OS};
      DEBIAN:
        sh: >-
          if [[ "{{.OS}}" == *"debian"* ]]; then echo "yes"; else echo "no"; fi
      FEDORA:
        sh: >-
          if [[ "{{.OS}}" == *"fedora"* ]]; then echo "yes"; else echo "no"; fi
      SUSE:
        sh: >-
          if [[ "{{.OS}}" == *"suse"* ]]; then echo "yes"; else echo "no"; fi
    cmds:
      - >-
        {{if eq .DEBIAN "yes" -}}
        sudo apt-get install -y cryptsetup dmsetup
        {{- end}}
      - >-
        {{if eq .FEDORA "yes" -}}
        sudo yum install -y cryptsetup device-mapper
        {{- end}}
      - >-
        {{if eq .SUSE "yes" -}}
        sudo zypper install -y cryptsetup device-mapper
        {{- end}}

  longhorn:prepare:check:
    platforms: [linux]
    preconditions:
      - kubectl version
      - jq --version
      - mktemp --version
      - sort --version
      - printf "Found printf"
    requires:
      vars:
        - KUBECONFIG
    cmds:
      - export KUBECONFIG={{.KUBECONFIG}}; curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.7.1/scripts/environment_check.sh | bash

  longhorn:deploy:self:
    vars:
      _VERSION: '1.7.1'
      _RELEASE: 'longhorn'
      _DATA_DIR: >-
        {{if .DATA_DIR -}}
        {{.DATA_DIR}}
        {{- else -}}
        /var/lib/longhorn/
        {{- end}}
    cmds:
      - helm repo add longhorn https://charts.longhorn.io
      - helm repo update
      - >
        helm upgrade --install
        {{._RELEASE}} longhorn/longhorn
        --version {{._VERSION}}
        --namespace longhorn-system --create-namespace
        --reuse-values
        --set defaultSettings.defaultDataPath={{._DATA_DIR}}
        --set defaultSettings.storageMinimalAvailablePercentage=12
        --set defaultSettings.defaultReplicaCount=1
        --set defaultSettings.backupTarget=s3://longhorn@us-east-1/
        --set longhornUI.replicas=1
        --set persistence.migratable=true
        --set csi.attacherReplicaCount=1
        --set csi.provisionerReplicaCount=1
        --set csi.resizerReplicaCount=1
        --set csi.snapshotterReplicaCount=1
        {{.CLI_ARGS}}

  longhorn:backup:s3:
    desc: >-
      Create secret of longhorn backup target
      <AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_ENDPOINTS>
      [VIRTUAL_HOSTED_STYLE]
    requires:
      vars:
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - AWS_ENDPOINTS # http(s)://host:port
    vars:
      _SECRET: >-
        {{if .SECRET -}}
        {{.SECRET}}
        {{- else -}}
        longhorn-backup-s3-compatible-secret
        {{- end}}
      _VIRTUAL_HOSTED_STYLE: >-
        {{if eq .VIRTUAL_HOSTED_STYLE "false" -}}
        false
        {{- else -}}
        true
        {{- end}}
    cmds:
      - >-
        kubectl create secret generic {{._SECRET}}
        -n longhorn-system
        --type Opaque
        --from-literal AWS_ACCESS_KEY_ID={{.AWS_ACCESS_KEY_ID}}
        --from-literal AWS_SECRET_ACCESS_KEY={{.AWS_SECRET_ACCESS_KEY}}
        --from-literal AWS_ENDPOINTS={{.AWS_ENDPOINTS}}
        --from-literal VIRTUAL_HOSTED_STYLE={{._VIRTUAL_HOSTED_STYLE}}
        {{.CLI_ARGS}}

  longhorn:backup:s3:update:
    desc: >-
      Update secret of longhorn backup target
      <AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_ENDPOINTS>
      [VIRTUAL_HOSTED_STYLE]
    requires:
      vars:
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - AWS_ENDPOINTS # http(s)://host:port
    vars:
      _SECRET: >-
        {{if .SECRET -}}
        {{.SECRET}}
        {{- else -}}
        longhorn-backup-s3-compatible-secret
        {{- end}}
      AWS_ACCESS_KEY_ID_BASE64:
        sh: 'echo -n {{.AWS_ACCESS_KEY_ID}} | base64'
      AWS_SECRET_ACCESS_KEY_BASE64:
        sh: 'echo -n {{.AWS_SECRET_ACCESS_KEY}} | base64'
      AWS_ENDPOINTS_BASE64:
        sh: 'echo -n {{.AWS_ENDPOINTS}} | base64'
      VIRTUAL_HOSTED_STYLE_BASE64: >-
        {{if eq .VIRTUAL_HOSTED_STYLE "false" -}}
        ZmFsc2U=
        {{- else -}}
        dHJ1ZQ==
        {{- end}}
    cmds:
      - |-
        kubectl apply -f - <<'EOF'
        apiVersion: v1
        kind: Secret
        metadata:
          name: {{._SECRET}}
          namespace: longhorn-system
        type: Opaque
        data:
          AWS_ACCESS_KEY_ID: {{.AWS_ACCESS_KEY_ID_BASE64}}
          AWS_SECRET_ACCESS_KEY: '{{.AWS_SECRET_ACCESS_KEY_BASE64}}'
          AWS_ENDPOINTS: '{{.AWS_ENDPOINTS_BASE64}}'
          VIRTUAL_HOSTED_STYLE: '{{.VIRTUAL_HOSTED_STYLE_BASE64}}'
        EOF

  metallb:deploy:self:
    cmds:
      - helm repo add --force-update metallb https://metallb.github.io/metallb
      - >-
        helm upgrade --install
        metallb metallb/metallb
        -n metallb-system --create-namespace
        {{.CLI_ARGS}}

  metallb:deploy:self:bitnami:
    cmds:
      - >-
        helm upgrade --install
        metallb oci://registry-1.docker.io/bitnamicharts/metallb
        -n metallb-system --create-namespace
        {{.CLI_ARGS}}

  metallb:deploy:pool:
    vars:
      _NAME: '{{default "metallb-pool" .NAME}}'
      _NAMESPACE: '{{default "metallb-system" .NAMESPACE}}'
      _CIDR: '{{default "192.168.10.0/24" .CIDR}}'
    cmds:
      - |-
        kubectl apply -f - <<'EOF'
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          # A name for the address pool. Services can request allocation
          # from a specific address pool using this name.
          name: {{._NAME}}
          namespace: {{._NAMESPACE}}
        spec:
          # A list of IP address ranges over which MetalLB has
          # authority. You can list multiple ranges in a single pool, they
          # will all share the same settings. Each range can be either a
          # CIDR prefix, or an explicit start-end range of IPs.
          addresses:
          - {{._CIDR}}
          # - 192.168.9.1-192.168.9.5
          # - 192.168.10.0/24
          # - fc00:f853:0ccd:e799::/124
        EOF

  metallb:deploy:ad:
    vars:
      _NAME: '{{default "metallb-ad" .NAME}}'
      _NAMESPACE: '{{default "metallb-system" .NAMESPACE}}'
      _POOL: '{{default "metallb-pool" .POOL}}'
    cmds:
      - |-
        kubectl apply -f - <<'EOF'
        kind: L2Advertisement
        apiVersion: metallb.io/v1beta1
        metadata:
          name: {{._NAME}}
          namespace: {{._NAMESPACE}}
        spec:
          ipAddressPools: [{{._POOL}}]
        EOF

  nginx:deploy:self:
    cmds:
      - >
        helm upgrade --install
        ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx
        --namespace ingress-nginx --create-namespace

  nuclio:deploy:self:
    cmds:
      - helm repo add nuclio https://nuclio.github.io/nuclio/charts --force-update
      - >-
        helm upgrade --install
        nuclio nuclio/nuclio
        --namespace nuclio --create-namespace
        --reuse-values
        {{.CLI_ARGS}}

  openebs:deploy:self:
    desc: >-
      [
      VERSION

      IS_ONLY_LOCAL_ENGINE
      LOCAL_ENGINE_DIR
      ]
    vars:
      _VERSION: '{{default "4.1.1" .VERSION}}'
      _ENGINE: >-
        {{if eq .IS_ONLY_LOCAL_ENGINE "true" -}}
        --set engines.replicated.mayastor.enabled=false
        {{- end}}

        --set hostpathClass.basePath='{{default "/var/openebs/local" .LOCAL_ENGINE_DIR}}'
    cmds:
      - helm repo add openebs https://openebs.github.io/openebs --force-update
      - >-
        helm upgrade --install
        openebs openebs/openebs
        --version {{._VERSION}}
        -n openebs --create-namespace
        --reuse-values
        {{_ENGINE}}
        {{.CLI_ARGS}}

  openfaas:deploy:self:
    cmds:
      - kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml
      - helm repo add openfaas https://openfaas.github.io/faas-netes/
      - helm repo update
      - >
        helm upgrade --install
        openfaas openfaas/openfaas
        --namespace openfaas --create-namespace

  prometheus:deploy:community:cnpg:
    cmds:
      - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
      - helm repo update
      - >
        helm upgrade --install
        prometheus-community prometheus-community/kube-prometheus-stack
        --values https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/main/docs/src/samples/monitoring/kube-stack-config.yaml

  rainbond:deploy:self:
    desc: >-
      [
      K3S
      CONTAINERD
      ]
      -- [helm_upgrade_options]
    vars:
      _K3S: >-
        {{if eq .K3S "yes" -}}
        --set useK3sContainerd=true
        {{- end}}
      _CONTAINERD: >-
        {{if and (eq .CONTAINERD "yes") (ne .K3S "yes") -}}
        --set operator.env[0].name=CONTAINER_RUNTIME
        --set operator.env[0].value=containerd
        {{- end}}
    cmds:
      - helm repo add rainbond https://openchart.goodrain.com/goodrain/rainbond --force-update
      - >-
        helm upgrade --install
        rainbond rainbond/rainbond-cluster
        -n rbd-system --create-namespace
        --reuse-values
        {{._K3S}}
        {{._CONTAINERD}}
        {{.CLI_ARGS}}

  rainbond:deploy:self:k3s:
    cmds:
      - task: rainbond:deploy:self
        vars:
          K3S: "yes"

  rancher:deploy:self:
    vars:
      _IP: '{{default "127.0.0.1" .IP}}'
    cmds:
      - helm repo add rancher-latest https://releases.rancher.com/server-charts/latest --force-update
      - >-
        helm upgrade --install
        rancher rancher-latest/rancher
        --namespace cattle-system --create-namespace
        --reuse-values
        --set replicas=1
        --set hostname={{._IP}}.sslip.io

  rclone:csi:deploy:self:
    vars:
      _VERSION: '{{if .VERSION}}{{.VERSION}}{{else}}1.20{{end}}'
    cmds:
      - git clone https://github.com/wunderio/csi-rclone
      - kubectl apply -f csi-rclone/deploy/kubernetes/{{._VERSION}}/
      - rm -r csi-rclone
      - kubectl delete storageclass rclone
      - |-
        kubectl apply -f - <<'EOF'
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
          name: rclone
        provisioner: csi-rclone
        parameters:
          pathPattern: "${.PVC.namespace}/${.PVC.annotations.csi-rclone/storage-path}"
        EOF

  rclone:csi:deploy:remote:default:s3:r2:
    cmds:
      - task: rclone:csi:deploy:remote:default:s3
        vars:
          PROVIDER: Cloudflare

  rclone:csi:deploy:remote:default:s3:
    requires:
      vars:
        - S3_ENDPOINT # http(s)://host:port
        - ACCESS_KEY_ID
        - SECRET_ACCESS_KEY
    vars:
      _PROVIDER: '{{if .PROVIDER}}{{.PROVIDER}}{{else}}Other{{end}}'
      _DIR: '{{if .DIR}}{{.DIR}}{{else}}{{end}}'
    cmds:
      - |-
        kubectl apply -n csi-rclone -f - <<'EOF'
        apiVersion: v1
        kind: Secret
        metadata:
          name: rclone-secret
        type: Opaque
        stringData:
          remote: "default"
          remotePath: "projectname"
          configData: |
            [default]
            type = s3
            provider = {{._PROVIDER}}
            access_key_id = {{.ACCESS_KEY_ID}}
            secret_access_key = {{.SECRET_ACCESS_KEY}}
            endpoint = {{.S3_ENDPOINT}}
            encoding = Slash
        EOF

  rook:deploy:self:
    cmds:
      - task: rook:deploy:operator
      # - task: rook:deploy:cluster

  rook:deploy:operator:
    cmds:
      - helm repo add rook-release https://charts.rook.io/release --force-update
      - >-
        helm upgrade --install
        rook-ceph rook-release/rook-ceph
        -n rook-ceph --create-namespace
        --reuse-values
        --set resources.requests.cpu=100m
        --set resources.requests.memory=128Mi
        --set csi.provisionerReplicas=1
      # --set csi.enableCSIEncryption=true
      # --wait

  rook:deploy:cluster:
    cmds:
      - helm repo add rook-release https://charts.rook.io/release --force-update
      - >-
        helm upgrade --install
        rook-ceph-cluster rook-release/rook-ceph-cluster
        --create-namespace --namespace rook-ceph
        --set cephClusterSpec.dataDirHostPath=/var/lib/rook
        --set cephClusterSpec.mon.count=1
        --set cephClusterSpec.mon.allowMultiplePerNode=false
        --set cephClusterSpec.mgr.count=1
        --set cephClusterSpec.mgr.allowMultiplePerNode=false
        --set cephClusterSpec.resources.mgr.limits.memory
        --set cephClusterSpec.resources.mgr.requests.cpu
        --set cephClusterSpec.resources.mgr.requests.memory
        --set cephClusterSpec.resources.mgr-sidecar.limits.memory
        --set cephClusterSpec.resources.mgr-sidecar.requests.cpu
        --set cephClusterSpec.resources.mgr-sidecar.requests.memory
        --set cephClusterSpec.resources.mon.limits.memory
        --set cephClusterSpec.resources.mon.requests.cpu
        --set cephClusterSpec.resources.mon.requests.memory
        --set cephClusterSpec.resources.osd.limits.memory
        --set cephClusterSpec.resources.osd.requests.cpu
        --set cephClusterSpec.resources.osd.requests.memory
        --set cephClusterSpec.resources.crashcollector.limits.memory
        --set cephClusterSpec.resources.crashcollector.requests.cpu
        --set cephClusterSpec.resources.crashcollector.requests.memory
        --set cephClusterSpec.resources.logcollector.limits.memory
        --set cephClusterSpec.resources.logcollector.requests.cpu
        --set cephClusterSpec.resources.logcollector.requests.memory
        --set cephClusterSpec.resources.cleanup.limits.memory
        --set cephClusterSpec.resources.cleanup.requests.cpu
        --set cephClusterSpec.resources.cleanup.requests.memory
        --set cephClusterSpec.resources.exporter.limits.memory
        --set cephClusterSpec.resources.exporter.requests.cpu
        --set cephClusterSpec.resources.exporter.requests.memory
      # TODO: cephBlockPools and after

  traefik:deploy:self:
    desc: >-
      Deploy traefik
      [
      RELEASE
      VERSION

      IS_DAEMONSET
      IS_NODEPORT
      IS_HOSTPORT

      IS_CRD
      IS_INGRESS
      IS_DEFAULT_INGRESS
      IS_GATEWAY_API

      IS_PROMETHEUS
      ]
      -- [options]
    vars:
      _RELEASE: '{{default "traefik" .RELEASE}}'
      _VERSION: '{{default "32.0.0" .VERSION}}'
      _NAMESPACE: '--namespace {{default "traefik" .NAMESPACE}} --create-namespace'
      _ENTRYPOINT: >-
        helm upgrade --install
        {{._RELEASE}} traefik/traefik --version {{._VERSION}}
        {{._NAMESPACE}}
        {{.CLI_ARGS}}
    cmds:
      - helm repo add traefik https://traefik.github.io/charts --force-update
      - |-
        {{._ENTRYPOINT}} --values - <<EOF
        deployment:
          kind: {{if eq .IS_DAEMONSET "false"}}Deployment{{else}}DaemonSet{{end}}

        service:
          type: {{default "NodePort" .SERVICE_TYPE}}

        ports:
          web:
            {{- if eq .IS_HOSTPORT "true"}}
            hostPort: 80
            {{- end}}
          websecure:
            {{- if eq .IS_HOSTPORT "true"}}
            hostPort: 443
            {{- end}}
            http3:
              enabled: true

        providers:
          kubernetesGateway:
            enabled: {{if eq .IS_GATEWAY_API "false"}}false{{else}}true{{end}}
            experimentalChannel: true

          kubernetesIngress:
            allowExternalNameServices: {{if eq .IS_INGRESS "false"}}false{{else}}true{{end}}

        {{if eq .CRD "false" -}}
          kubernetesCRD:
            enabled: false

        {{- else -}}
          kubernetesCRD:
            allowCrossNamespace: true
            allowExternalNameServices: true

        ingressRoute:
          dashboard:
            enabled: true
        {{- end}}

        ingressClass:
          isDefaultClass: {{if eq .IS_DEFAULT_INGRESS "false"}}false{{else}}true{{end}}

        gateway:
          listeners:
            web:
              namespacePolicy: All
            websecure:
              port: 8443
              protocol: HTTPS
              namespacePolicy: All
              mode: Terminate
              certificateRefs:
                - kind: Secret
                  name: master-general-secret
                  namespace: infra

        metrics:
          prometheus:
            enabled: {{if eq .IS_PROMETHEUS "false"}}false{{else}}true{{end}}
            service:
              enabled: true
        EOF

  tsuru:deploy:self:
    cmds:
      - helm repo add tsuru https://tsuru.github.io/charts --force-update
      - >
        helm upgrade --install
        tsuru tsuru/tsuru-stack
        --create-namespace --namespace tsuru-system
        --set ingress-nginx.enabled=true
        --set tsuru-api.service.type=NodePort

  velero:deploy:self:
    desc: 'Deploy velero 1.14.1'
    vars:
      _VERSION: '{{default "7.2.1" .VERSION}}'
      _PROVIDERS: >-
        --set initContainers[0].name=velero-plugin-for-aws
        --set initContainers[0].image=velero/velero-plugin-for-aws:v1.10.1
        --set initContainers[0].imagePullPolicy=IfNotPresent
        --set initContainers[0].volumeMounts[0].mountPath=/target
        --set initContainers[0].volumeMounts[0].name=plugins
      _LOCATION: >-
        --set backupsEnabled=true
        --set configuration.volumeSnapshotLocation=[]
        --set configuration.backupStorageLocation=[]
      # kopia/restic
      _FSB: >-
        --set deployNodeAgent=true
        --set configuration.defaultVolumesToFsBackup=true
        --set configuration.uploaderType=kopia
      _RESOURCES: >-
        --set resources.requests.cpu=100m
        --set resources.requests.memory=100Mi
        --set resources.limits.cpu=300m
        --set resources.limits.memory=300Mi
        --set nodeAgent.resources.requests.cpu=100m
        --set nodeAgent.resources.requests.memory=100Mi
        --set nodeAgent.resources.limits.cpu=400m
        --set nodeAgent.resources.limits.memory=400Mi
    cmds:
      - helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts --force-update
      - >
        helm upgrade --install
        velero vmware-tanzu/velero
        --version {{._VERSION}}
        -n velero --create-namespace
        --reuse-values
        {{._PROVIDERS}}
        {{._LOCATION}}
        {{._FSB}}
        {{._RESOURCES}}
        {{.CLI_ARGS}}

  velero:repo:passwd:
    dir: '{{.USER_WORKING_DIR}}'
    requires:
      vars: [PASSWORD]
    vars:
      _PASSWORD_BASE64:
        sh: >-
          echo -n '{{default "static-passw0rd" .PASSWORD}}' | base64
    cmds:
      - |-
        cat > temp-manifest.yaml <<EOF
        data:
          repository-password: '{{._PASSWORD_BASE64}}'
        EOF
      - >-
        kubectl patch
        Secret velero-repo-credentials
        -n velero
        --patch-file temp-manifest.yaml
      - defer: rm temp-manifest.yaml

  velero:location:create:s3:
    desc: >-
      <
      AWS_ENDPOINTS
      AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_BUCKET
      >
      [
      NAME
      NAMESPACE
      AWS_REGION
      SECRET_NAME
      SECRET_KEY
      IS_DEFAULT
      ]
      -- [options]
    requires:
      vars:
        - AWS_ENDPOINTS
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - AWS_BUCKET
    vars:
      _NAME: '{{default "s3-backup" .NAME}}'
      _CRED_BASE64:
        sh: |-
          base64 -w 0 <<EOF
          [default]
          aws_access_key_id = {{.AWS_ACCESS_KEY_ID}}
          aws_secret_access_key = {{.AWS_SECRET_ACCESS_KEY}}
          EOF
      _S3: >-
        --provider aws
        --bucket {{.AWS_BUCKET}}
        --credential {{default "velero-location-credentials" .SECRET_NAME}}={{default "s3" .SECRET_KEY}}
        --config s3Url={{.AWS_ENDPOINTS}}
        --config s3ForcePathStyle="true"
        --config region={{default "us-east-1" .AWS_REGION}}
        --config checksumAlgorithm=""
      _DEFAULT: >-
        {{if eq .IS_DEFAULT "true" -}}
        --default
        {{- end}}
    cmds:
      - |-
        kubectl apply -f - <<EOF
        apiVersion: v1
        kind: Secret
        metadata:
          name: {{default "velero-location-credentials" .SECRET_NAME}}
          namespace: {{default "velero" .NAMESPACE}}
        type: Opaque
        data:
          {{default "s3" .SECRET_KEY}}: {{._CRED_BASE64}}
        EOF
      - >-
        velero backup-location create {{._NAME}}
        {{._S3}}
        {{._DEFAULT}}
        {{.CLI_ARGS}}

  velero:schedule:create:
    cmds:
      - >-
        velero schedule create {{.NAME}}
        --schedule='{{default "0 */10 * * *" .CRON}}'
        --default-volumes-to-fs-backup=true
        {{if .LOCATION}}--storage-location {{.LOCATION}}{{end}}
        --ttl {{default "30d" .TTL}}
        {{.CLI_ARGS}}
